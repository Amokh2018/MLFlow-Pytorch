{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment <br>\n",
    "<br>\n",
    "• Torch - 1.6.0<br>\n",
    "• Torchvision – 0.7.0<br>\n",
    "• CUDA – 10.1<br>\n",
    "• CuDNN – v7.6.5.32 for CUDA 10.1<br>\n",
    "• Sklearn – 0.22.2.post1<br>\n",
    "• MLFlow – 1.10.0<br>\n",
    "• numpy – 1.18.5<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import numpy as np \n",
    "import mlflow\n",
    "import mlflow.pytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.0.0\n",
      "torchvision: 0.15.1\n",
      "sklearn: 1.2.2\n",
      "MLFlow: 2.2.2\n",
      "Numpy: 1.23.5\n",
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch: {}\".format(torch.__version__))\n",
    "print(\"torchvision: {}\".format(torchvision.__version__))\n",
    "print(\"sklearn: {}\".format(sklearn.__version__))\n",
    "print(\"MLFlow: {}\".format(mlflow.__version__))\n",
    "print(\"Numpy: {}\".format(np.__version__))\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available()\n",
    "else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define basic Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_classes = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13.9%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "73.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "test_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=None)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train_set.data, train_set.targets\n",
    "x_test, y_test = test_set.data, test_set.targets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the format of the data should be (m, c, h, w), where m stands for the number of samples, c stands for the number of channels, h stands for the height of the samples, and w stands for the width of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0],1, x_train.shape[1], x_train.shape[2])\n",
    "x_test = x_test.reshape(x_test.shape[0],1, x_test.shape[1], x_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_one_hot(num_classes, labels):\n",
    "    one_hot = torch.zeros(([labels.shape[0], num_classes])) \n",
    "    for f in range(len(labels)):\n",
    "        one_hot[f][labels[f]] = 1 \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_one_hot(num_classes, y_train)\n",
    "y_test = to_one_hot(num_classes, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      "x_train: torch.Size([60000, 1, 28, 28])\n",
      "y_train: torch.Size([60000, 10])\n",
      "x_test: torch.Size([10000, 1, 28, 28])\n",
      "y_test: torch.Size([10000, 10])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shapes\")\n",
    "print(\"x_train: {}\\ny_train: {}\".format(x_train.shape,\n",
    "y_train.shape))\n",
    "print(\"x_test: {}\\ny_test: {}\".format(x_test.shape,\n",
    "y_test.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLFlow Run- Training and Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(model, self).__init__()\n",
    "        # IN 1x28x28 OUT 16x14x14\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=2, padding=1, dilation=1)\n",
    "        # IN 16x14x14 OUT 32x6x6\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=0, dilation=1)\n",
    "        # IN 32x6x6 OUT 64x2x2\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=0, dilation=1)\n",
    "        # IN 64x2x2 OUT 256\n",
    "        self.flat1 = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.dense2 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.dense3 = nn.Linear(in_features=64, out_features=10)\n",
    "\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.conv1(x) \n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv3(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.flat1(x)\n",
    "        x = self.dense1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.dense2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.dense3(x)\n",
    "        x = nn.Softmax()(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.TensorDataset(x_train,y_train)\n",
    "train_loader = data.DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/lvr6c0md2rb5v_rb1ngcc27h0000gn/T/ipykernel_10277/2201976014.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = nn.Softmax()(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch_Num 0 Loss 0.008782649412751198\n",
      "Epoch 0 Batch_Num 1 Loss 0.007078584283590317\n",
      "Epoch 0 Batch_Num 2 Loss 0.0029490473680198193\n",
      "Epoch 0 Batch_Num 3 Loss 0.008408360183238983\n",
      "Epoch 0 Batch_Num 4 Loss 0.0056398650631308556\n",
      "Epoch 0 Batch_Num 5 Loss 0.009857377037405968\n",
      "Epoch 0 Batch_Num 6 Loss 0.009978807531297207\n",
      "Epoch 0 Batch_Num 7 Loss 0.0054803951643407345\n",
      "Epoch 0 Batch_Num 8 Loss 0.003967866767197847\n",
      "Epoch 0 Batch_Num 9 Loss 0.00782245397567749\n",
      "Epoch 0 Batch_Num 10 Loss 0.007897497154772282\n",
      "Epoch 0 Batch_Num 11 Loss 0.008148512803018093\n",
      "Epoch 0 Batch_Num 12 Loss 0.0014904446434229612\n",
      "Epoch 0 Batch_Num 13 Loss 0.009080146439373493\n",
      "Epoch 0 Batch_Num 14 Loss 0.012505291029810905\n",
      "Epoch 0 Batch_Num 15 Loss 0.005822139326483011\n",
      "Epoch 0 Batch_Num 16 Loss 0.004930921830236912\n",
      "Epoch 0 Batch_Num 17 Loss 0.005538105498999357\n",
      "Epoch 0 Batch_Num 18 Loss 0.00564067717641592\n",
      "Epoch 0 Batch_Num 19 Loss 0.009290741756558418\n",
      "Epoch 0 Batch_Num 20 Loss 0.00917443074285984\n",
      "Epoch 0 Batch_Num 21 Loss 0.0015712324529886246\n",
      "Epoch 0 Batch_Num 22 Loss 0.006772083695977926\n",
      "Epoch 0 Batch_Num 23 Loss 0.003987899981439114\n",
      "Epoch 0 Batch_Num 24 Loss 0.007711862213909626\n",
      "Epoch 0 Batch_Num 25 Loss 0.0031235716305673122\n",
      "Epoch 0 Batch_Num 26 Loss 0.005225016735494137\n",
      "Epoch 0 Batch_Num 27 Loss 0.00999431125819683\n",
      "Epoch 0 Batch_Num 28 Loss 0.008046591654419899\n",
      "Epoch 0 Batch_Num 29 Loss 0.004241916351020336\n",
      "Epoch 0 Batch_Num 30 Loss 0.0063294703140854836\n",
      "Epoch 0 Batch_Num 31 Loss 0.0057638040743768215\n",
      "Epoch 0 Batch_Num 32 Loss 0.0059102182276546955\n",
      "Epoch 0 Batch_Num 33 Loss 0.007845423184335232\n",
      "Epoch 0 Batch_Num 34 Loss 0.008879968896508217\n",
      "Epoch 0 Batch_Num 35 Loss 0.006348940543830395\n",
      "Epoch 0 Batch_Num 36 Loss 0.004098966252058744\n",
      "Epoch 0 Batch_Num 37 Loss 0.004018825478851795\n",
      "Epoch 0 Batch_Num 38 Loss 0.001802199287340045\n",
      "Epoch 0 Batch_Num 39 Loss 0.005554293282330036\n",
      "Epoch 0 Batch_Num 40 Loss 0.00467683793976903\n",
      "Epoch 0 Batch_Num 41 Loss 0.0025280145928263664\n",
      "Epoch 0 Batch_Num 42 Loss 0.008750414475798607\n",
      "Epoch 0 Batch_Num 43 Loss 0.004838086664676666\n",
      "Epoch 0 Batch_Num 44 Loss 0.0014897638466209173\n",
      "Epoch 0 Batch_Num 45 Loss 0.0064319223165512085\n",
      "Epoch 0 Batch_Num 46 Loss 0.015796130523085594\n",
      "Epoch 0 Batch_Num 47 Loss 0.0026876390911638737\n",
      "Epoch 0 Batch_Num 48 Loss 0.003704262198880315\n",
      "Epoch 0 Batch_Num 49 Loss 0.011428275145590305\n",
      "Epoch 0 Batch_Num 50 Loss 0.004105289001017809\n",
      "Epoch 0 Batch_Num 51 Loss 0.008422167040407658\n",
      "Epoch 0 Batch_Num 52 Loss 0.008160987868905067\n",
      "Epoch 0 Batch_Num 53 Loss 0.006377082318067551\n",
      "Epoch 0 Batch_Num 54 Loss 0.0028637039940804243\n",
      "Epoch 0 Batch_Num 55 Loss 0.004542945418506861\n",
      "Epoch 0 Batch_Num 56 Loss 0.009128781966865063\n",
      "Epoch 0 Batch_Num 57 Loss 0.004384559113532305\n",
      "Epoch 0 Batch_Num 58 Loss 0.0022676223888993263\n",
      "Epoch 0 Batch_Num 59 Loss 0.00960140023380518\n",
      "Epoch 0 Batch_Num 60 Loss 0.007072796113789082\n",
      "Epoch 0 Batch_Num 61 Loss 0.009560612961649895\n",
      "Epoch 0 Batch_Num 62 Loss 0.006618828512728214\n",
      "Epoch 0 Batch_Num 63 Loss 0.004880527965724468\n",
      "Epoch 0 Batch_Num 64 Loss 0.003176732687279582\n",
      "Epoch 0 Batch_Num 65 Loss 0.00957866944372654\n",
      "Epoch 0 Batch_Num 66 Loss 0.010520930401980877\n",
      "Epoch 0 Batch_Num 67 Loss 0.006743353791534901\n",
      "Epoch 0 Batch_Num 68 Loss 0.0026478427462279797\n",
      "Epoch 0 Batch_Num 69 Loss 0.0035475059412419796\n",
      "Epoch 0 Batch_Num 70 Loss 0.006596615072339773\n",
      "Epoch 0 Batch_Num 71 Loss 0.008831256069242954\n",
      "Epoch 0 Batch_Num 72 Loss 0.003132713958621025\n",
      "Epoch 0 Batch_Num 73 Loss 0.005884475074708462\n",
      "Epoch 0 Batch_Num 74 Loss 0.00293371663428843\n",
      "Epoch 0 Batch_Num 75 Loss 0.00815893430262804\n",
      "Epoch 0 Batch_Num 76 Loss 0.002374757779762149\n",
      "Epoch 0 Batch_Num 77 Loss 0.0017970692133530974\n",
      "Epoch 0 Batch_Num 78 Loss 0.014926205389201641\n",
      "Epoch 0 Batch_Num 79 Loss 0.005241683684289455\n",
      "Epoch 0 Batch_Num 80 Loss 0.004554372746497393\n",
      "Epoch 0 Batch_Num 81 Loss 0.002440647454932332\n",
      "Epoch 0 Batch_Num 82 Loss 0.002991971094161272\n",
      "Epoch 0 Batch_Num 83 Loss 0.003310366300866008\n",
      "Epoch 0 Batch_Num 84 Loss 0.008851254358887672\n",
      "Epoch 0 Batch_Num 85 Loss 0.0021342511754482985\n",
      "Epoch 0 Batch_Num 86 Loss 0.003180247498676181\n",
      "Epoch 0 Batch_Num 87 Loss 0.0009820001432672143\n",
      "Epoch 0 Batch_Num 88 Loss 0.009109992533922195\n",
      "Epoch 0 Batch_Num 89 Loss 0.0010684606386348605\n",
      "Epoch 0 Batch_Num 90 Loss 0.001560855540446937\n",
      "Epoch 0 Batch_Num 91 Loss 0.0008532515494152904\n",
      "Epoch 0 Batch_Num 92 Loss 0.007726720534265041\n",
      "Epoch 0 Batch_Num 93 Loss 0.004622466862201691\n",
      "Epoch 0 Batch_Num 94 Loss 0.0020458977669477463\n",
      "Epoch 0 Batch_Num 95 Loss 0.0009306562133133411\n",
      "Epoch 0 Batch_Num 96 Loss 0.005467499140650034\n",
      "Epoch 0 Batch_Num 97 Loss 0.002272603567689657\n",
      "Epoch 0 Batch_Num 98 Loss 0.002418173709884286\n",
      "Epoch 0 Batch_Num 99 Loss 0.0071570551954209805\n",
      "Epoch 0 Batch_Num 100 Loss 0.004277174826711416\n",
      "Epoch 0 Batch_Num 101 Loss 0.00747319171205163\n",
      "Epoch 0 Batch_Num 102 Loss 0.0025573745369911194\n",
      "Epoch 0 Batch_Num 103 Loss 0.015596074052155018\n",
      "Epoch 0 Batch_Num 104 Loss 0.004183170385658741\n",
      "Epoch 0 Batch_Num 105 Loss 0.002360677346587181\n",
      "Epoch 0 Batch_Num 106 Loss 0.008141746744513512\n",
      "Epoch 0 Batch_Num 107 Loss 0.008634405210614204\n",
      "Epoch 0 Batch_Num 108 Loss 0.006062653847038746\n",
      "Epoch 0 Batch_Num 109 Loss 0.004581059329211712\n",
      "Epoch 0 Batch_Num 110 Loss 0.0074438005685806274\n",
      "Epoch 0 Batch_Num 111 Loss 0.009860374964773655\n",
      "Epoch 0 Batch_Num 112 Loss 0.003721129149198532\n",
      "Epoch 0 Batch_Num 113 Loss 0.003618720220401883\n",
      "Epoch 0 Batch_Num 114 Loss 0.006013792008161545\n",
      "Epoch 0 Batch_Num 115 Loss 0.00384673778899014\n",
      "Epoch 0 Batch_Num 116 Loss 0.0036982796154916286\n",
      "Epoch 0 Batch_Num 117 Loss 0.006929302122443914\n",
      "Epoch 0 Batch_Num 118 Loss 0.003953760024160147\n",
      "Epoch 0 Batch_Num 119 Loss 0.003352826926857233\n",
      "Epoch 0 Batch_Num 120 Loss 0.002725734608247876\n",
      "Epoch 0 Batch_Num 121 Loss 0.006822624709457159\n",
      "Epoch 0 Batch_Num 122 Loss 0.008485691621899605\n",
      "Epoch 0 Batch_Num 123 Loss 0.008933117613196373\n",
      "Epoch 0 Batch_Num 124 Loss 0.007520460989326239\n",
      "Epoch 0 Batch_Num 125 Loss 0.003585781902074814\n",
      "Epoch 0 Batch_Num 126 Loss 0.00478375842794776\n",
      "Epoch 0 Batch_Num 127 Loss 0.002357517136260867\n",
      "Epoch 0 Batch_Num 128 Loss 0.001703791320323944\n",
      "Epoch 0 Batch_Num 129 Loss 0.001412861282005906\n",
      "Epoch 0 Batch_Num 130 Loss 0.00408927584066987\n",
      "Epoch 0 Batch_Num 131 Loss 0.002001497894525528\n",
      "Epoch 0 Batch_Num 132 Loss 0.0011903527192771435\n",
      "Epoch 0 Batch_Num 133 Loss 0.0035241302102804184\n",
      "Epoch 0 Batch_Num 134 Loss 0.010885978117585182\n",
      "Epoch 0 Batch_Num 135 Loss 0.005796418525278568\n",
      "Epoch 0 Batch_Num 136 Loss 0.008322623558342457\n",
      "Epoch 0 Batch_Num 137 Loss 0.008123083040118217\n",
      "Epoch 0 Batch_Num 138 Loss 0.00630966667085886\n",
      "Epoch 0 Batch_Num 139 Loss 0.005969579331576824\n",
      "Epoch 0 Batch_Num 140 Loss 0.0038783340714871883\n",
      "Epoch 0 Batch_Num 141 Loss 0.006737838499248028\n",
      "Epoch 0 Batch_Num 142 Loss 0.006600230932235718\n",
      "Epoch 0 Batch_Num 143 Loss 0.00920018833130598\n",
      "Epoch 0 Batch_Num 144 Loss 0.008807196281850338\n",
      "Epoch 0 Batch_Num 145 Loss 0.001894779154099524\n",
      "Epoch 0 Batch_Num 146 Loss 0.013512049801647663\n",
      "Epoch 0 Batch_Num 147 Loss 0.010648039169609547\n",
      "Epoch 0 Batch_Num 148 Loss 0.0022192061878740788\n",
      "Epoch 0 Batch_Num 149 Loss 0.009748694486916065\n",
      "Epoch 0 Batch_Num 150 Loss 0.0038144432473927736\n",
      "Epoch 0 Batch_Num 151 Loss 0.006896808743476868\n",
      "Epoch 0 Batch_Num 152 Loss 0.0014083351707085967\n",
      "Epoch 0 Batch_Num 153 Loss 0.013444444164633751\n",
      "Epoch 0 Batch_Num 154 Loss 0.002887304639443755\n",
      "Epoch 0 Batch_Num 155 Loss 0.006753651890903711\n",
      "Epoch 0 Batch_Num 156 Loss 0.00791814923286438\n",
      "Epoch 0 Batch_Num 157 Loss 0.006149440072476864\n",
      "Epoch 0 Batch_Num 158 Loss 0.004028344992548227\n",
      "Epoch 0 Batch_Num 159 Loss 0.0037257366348057985\n",
      "Epoch 0 Batch_Num 160 Loss 0.006231491453945637\n",
      "Epoch 0 Batch_Num 161 Loss 0.015193218365311623\n",
      "Epoch 0 Batch_Num 162 Loss 0.006982538849115372\n",
      "Epoch 0 Batch_Num 163 Loss 0.005039116833359003\n",
      "Epoch 0 Batch_Num 164 Loss 0.005028841085731983\n",
      "Epoch 0 Batch_Num 165 Loss 0.0035252596717327833\n",
      "Epoch 0 Batch_Num 166 Loss 0.007009422872215509\n",
      "Epoch 0 Batch_Num 167 Loss 0.0035219048149883747\n",
      "Epoch 0 Batch_Num 168 Loss 0.008928551338613033\n",
      "Epoch 0 Batch_Num 169 Loss 0.004527136217802763\n",
      "Epoch 0 Batch_Num 170 Loss 0.008183347061276436\n",
      "Epoch 0 Batch_Num 171 Loss 0.003499655518680811\n",
      "Epoch 0 Batch_Num 172 Loss 0.0061064972542226315\n",
      "Epoch 0 Batch_Num 173 Loss 0.004086636006832123\n",
      "Epoch 0 Batch_Num 174 Loss 0.0015221904031932354\n",
      "Epoch 0 Batch_Num 175 Loss 0.0024645314551889896\n",
      "Epoch 0 Batch_Num 176 Loss 0.005510909948498011\n",
      "Epoch 0 Batch_Num 177 Loss 0.006614727433770895\n",
      "Epoch 0 Batch_Num 178 Loss 0.004749595187604427\n",
      "Epoch 0 Batch_Num 179 Loss 0.009065362624824047\n",
      "Epoch 0 Batch_Num 180 Loss 0.006726657040417194\n",
      "Epoch 0 Batch_Num 181 Loss 0.008857347071170807\n",
      "Epoch 0 Batch_Num 182 Loss 0.002678187098354101\n",
      "Epoch 0 Batch_Num 183 Loss 0.007659570313990116\n",
      "Epoch 0 Batch_Num 184 Loss 0.007475716061890125\n",
      "Epoch 0 Batch_Num 185 Loss 0.00351070542819798\n",
      "Epoch 0 Batch_Num 186 Loss 0.003168561728671193\n",
      "Epoch 0 Batch_Num 187 Loss 0.0036202394403517246\n",
      "Epoch 0 Batch_Num 188 Loss 0.0027407039888203144\n",
      "Epoch 0 Batch_Num 189 Loss 0.003540377365425229\n",
      "Epoch 0 Batch_Num 190 Loss 0.0024742395617067814\n",
      "Epoch 0 Batch_Num 191 Loss 0.008791313506662846\n",
      "Epoch 0 Batch_Num 192 Loss 0.0058860741555690765\n",
      "Epoch 0 Batch_Num 193 Loss 0.005391774233430624\n",
      "Epoch 0 Batch_Num 194 Loss 0.011938752606511116\n",
      "Epoch 0 Batch_Num 195 Loss 0.003412314224988222\n",
      "Epoch 0 Batch_Num 196 Loss 0.006960953585803509\n",
      "Epoch 0 Batch_Num 197 Loss 0.009204480797052383\n",
      "Epoch 0 Batch_Num 198 Loss 0.0037605916149914265\n",
      "Epoch 0 Batch_Num 199 Loss 0.002173398155719042\n",
      "Epoch 0 Batch_Num 200 Loss 0.00485405046492815\n",
      "Epoch 0 Batch_Num 201 Loss 0.0025850622914731503\n",
      "Epoch 0 Batch_Num 202 Loss 0.003508196445181966\n",
      "Epoch 0 Batch_Num 203 Loss 0.0037050347309559584\n",
      "Epoch 0 Batch_Num 204 Loss 0.005678075365722179\n",
      "Epoch 0 Batch_Num 205 Loss 0.0018145149806514382\n",
      "Epoch 0 Batch_Num 206 Loss 0.010248213075101376\n",
      "Epoch 0 Batch_Num 207 Loss 0.009829627349972725\n",
      "Epoch 0 Batch_Num 208 Loss 0.006674189120531082\n",
      "Epoch 0 Batch_Num 209 Loss 0.006361754145473242\n",
      "Epoch 0 Batch_Num 210 Loss 0.009463035501539707\n",
      "Epoch 0 Batch_Num 211 Loss 0.006743614561855793\n",
      "Epoch 0 Batch_Num 212 Loss 0.0014476681826636195\n",
      "Epoch 0 Batch_Num 213 Loss 0.0046526282094419\n",
      "Epoch 0 Batch_Num 214 Loss 0.00811534933745861\n",
      "Epoch 0 Batch_Num 215 Loss 0.0030794604681432247\n",
      "Epoch 0 Batch_Num 216 Loss 0.004802283830940723\n",
      "Epoch 0 Batch_Num 217 Loss 0.007278413511812687\n",
      "Epoch 0 Batch_Num 218 Loss 0.0033187910448759794\n",
      "Epoch 0 Batch_Num 219 Loss 0.006130100693553686\n",
      "Epoch 0 Batch_Num 220 Loss 0.0032336630392819643\n",
      "Epoch 0 Batch_Num 221 Loss 0.006685925181955099\n",
      "Epoch 0 Batch_Num 222 Loss 0.00300473440438509\n",
      "Epoch 0 Batch_Num 223 Loss 0.0011744803050532937\n",
      "Epoch 0 Batch_Num 224 Loss 0.0013437820598483086\n",
      "Epoch 0 Batch_Num 225 Loss 0.008819608017802238\n",
      "Epoch 0 Batch_Num 226 Loss 0.00393681088462472\n",
      "Epoch 0 Batch_Num 227 Loss 0.0009844533633440733\n",
      "Epoch 0 Batch_Num 228 Loss 0.0011269156821072102\n",
      "Epoch 0 Batch_Num 229 Loss 0.004731681197881699\n",
      "Epoch 0 Batch_Num 230 Loss 0.0016932211583480239\n",
      "Epoch 0 Batch_Num 231 Loss 0.0012079683365300298\n",
      "Epoch 0 Batch_Num 232 Loss 0.0027959332801401615\n",
      "Epoch 0 Batch_Num 233 Loss 0.011212749406695366\n",
      "Epoch 0 Batch_Num 234 Loss 0.013653619214892387\n",
      "Epoch 1 Batch_Num 0 Loss 0.008263101801276207\n",
      "Epoch 1 Batch_Num 1 Loss 0.004437409806996584\n",
      "Epoch 1 Batch_Num 2 Loss 0.006334325764328241\n",
      "Epoch 1 Batch_Num 3 Loss 0.010876075364649296\n",
      "Epoch 1 Batch_Num 4 Loss 0.003903999226167798\n",
      "Epoch 1 Batch_Num 5 Loss 0.0035455883480608463\n",
      "Epoch 1 Batch_Num 6 Loss 0.007724198512732983\n",
      "Epoch 1 Batch_Num 7 Loss 0.0011599219869822264\n",
      "Epoch 1 Batch_Num 8 Loss 0.007098029367625713\n",
      "Epoch 1 Batch_Num 9 Loss 0.007496281061321497\n",
      "Epoch 1 Batch_Num 10 Loss 0.008398449048399925\n",
      "Epoch 1 Batch_Num 11 Loss 0.010418524034321308\n",
      "Epoch 1 Batch_Num 12 Loss 0.00490616774186492\n",
      "Epoch 1 Batch_Num 13 Loss 0.00512488279491663\n",
      "Epoch 1 Batch_Num 14 Loss 0.006787873804569244\n",
      "Epoch 1 Batch_Num 15 Loss 0.004521534312516451\n",
      "Epoch 1 Batch_Num 16 Loss 0.002188278827816248\n",
      "Epoch 1 Batch_Num 17 Loss 0.004319685511291027\n",
      "Epoch 1 Batch_Num 18 Loss 0.004725760314613581\n",
      "Epoch 1 Batch_Num 19 Loss 0.007671820465475321\n",
      "Epoch 1 Batch_Num 20 Loss 0.006246995646506548\n",
      "Epoch 1 Batch_Num 21 Loss 0.0007335933623835444\n",
      "Epoch 1 Batch_Num 22 Loss 0.007898238487541676\n",
      "Epoch 1 Batch_Num 23 Loss 0.004374681040644646\n",
      "Epoch 1 Batch_Num 24 Loss 0.005533002316951752\n",
      "Epoch 1 Batch_Num 25 Loss 0.002679363824427128\n",
      "Epoch 1 Batch_Num 26 Loss 0.008931680582463741\n",
      "Epoch 1 Batch_Num 27 Loss 0.00541729899123311\n",
      "Epoch 1 Batch_Num 28 Loss 0.005182849243283272\n",
      "Epoch 1 Batch_Num 29 Loss 0.006842467002570629\n",
      "Epoch 1 Batch_Num 30 Loss 0.008809850551187992\n",
      "Epoch 1 Batch_Num 31 Loss 0.0031997598707675934\n",
      "Epoch 1 Batch_Num 32 Loss 0.005457667633891106\n",
      "Epoch 1 Batch_Num 33 Loss 0.002978249918669462\n",
      "Epoch 1 Batch_Num 34 Loss 0.005564096383750439\n",
      "Epoch 1 Batch_Num 35 Loss 0.005223735235631466\n",
      "Epoch 1 Batch_Num 36 Loss 0.008109022863209248\n",
      "Epoch 1 Batch_Num 37 Loss 0.002025182591751218\n",
      "Epoch 1 Batch_Num 38 Loss 0.003532248781993985\n",
      "Epoch 1 Batch_Num 39 Loss 0.002215147018432617\n",
      "Epoch 1 Batch_Num 40 Loss 0.005976679734885693\n",
      "Epoch 1 Batch_Num 41 Loss 0.0027015244122594595\n",
      "Epoch 1 Batch_Num 42 Loss 0.008141678757965565\n",
      "Epoch 1 Batch_Num 43 Loss 0.0022648642770946026\n",
      "Epoch 1 Batch_Num 44 Loss 0.0014649400254711509\n",
      "Epoch 1 Batch_Num 45 Loss 0.00910644419491291\n",
      "Epoch 1 Batch_Num 46 Loss 0.005884247366338968\n",
      "Epoch 1 Batch_Num 47 Loss 0.001423932728357613\n",
      "Epoch 1 Batch_Num 48 Loss 0.00563907902687788\n",
      "Epoch 1 Batch_Num 49 Loss 0.004491682164371014\n",
      "Epoch 1 Batch_Num 50 Loss 0.007922654040157795\n",
      "Epoch 1 Batch_Num 51 Loss 0.011282863095402718\n",
      "Epoch 1 Batch_Num 52 Loss 0.00618726247921586\n",
      "Epoch 1 Batch_Num 53 Loss 0.005729702766984701\n",
      "Epoch 1 Batch_Num 54 Loss 0.007851095870137215\n",
      "Epoch 1 Batch_Num 55 Loss 0.0063800811767578125\n",
      "Epoch 1 Batch_Num 56 Loss 0.009077472612261772\n",
      "Epoch 1 Batch_Num 57 Loss 0.004606225527822971\n",
      "Epoch 1 Batch_Num 58 Loss 0.0020347018726170063\n",
      "Epoch 1 Batch_Num 59 Loss 0.003031540662050247\n",
      "Epoch 1 Batch_Num 60 Loss 0.0018319390946999192\n",
      "Epoch 1 Batch_Num 61 Loss 0.00694255530834198\n",
      "Epoch 1 Batch_Num 62 Loss 0.006164289079606533\n",
      "Epoch 1 Batch_Num 63 Loss 0.003405680414289236\n",
      "Epoch 1 Batch_Num 64 Loss 0.002834197599440813\n",
      "Epoch 1 Batch_Num 65 Loss 0.010821497067809105\n",
      "Epoch 1 Batch_Num 66 Loss 0.009535546414554119\n",
      "Epoch 1 Batch_Num 67 Loss 0.009148218668997288\n",
      "Epoch 1 Batch_Num 68 Loss 0.004059869330376387\n",
      "Epoch 1 Batch_Num 69 Loss 0.006389097776263952\n",
      "Epoch 1 Batch_Num 70 Loss 0.003503119572997093\n",
      "Epoch 1 Batch_Num 71 Loss 0.004459611140191555\n",
      "Epoch 1 Batch_Num 72 Loss 0.0012724881526082754\n",
      "Epoch 1 Batch_Num 73 Loss 0.0017842635279521346\n",
      "Epoch 1 Batch_Num 74 Loss 0.004718461073935032\n",
      "Epoch 1 Batch_Num 75 Loss 0.0039892541244626045\n",
      "Epoch 1 Batch_Num 76 Loss 0.005244243424385786\n",
      "Epoch 1 Batch_Num 77 Loss 0.00604946818202734\n",
      "Epoch 1 Batch_Num 78 Loss 0.011174671351909637\n",
      "Epoch 1 Batch_Num 79 Loss 0.004938760306686163\n",
      "Epoch 1 Batch_Num 80 Loss 0.003987808711826801\n",
      "Epoch 1 Batch_Num 81 Loss 0.002284875139594078\n",
      "Epoch 1 Batch_Num 82 Loss 0.0018730458104982972\n",
      "Epoch 1 Batch_Num 83 Loss 0.0020195599645376205\n",
      "Epoch 1 Batch_Num 84 Loss 0.00398389995098114\n",
      "Epoch 1 Batch_Num 85 Loss 0.002515109721571207\n",
      "Epoch 1 Batch_Num 86 Loss 0.001986305695027113\n",
      "Epoch 1 Batch_Num 87 Loss 0.0019452028209343553\n",
      "Epoch 1 Batch_Num 88 Loss 0.009565236046910286\n",
      "Epoch 1 Batch_Num 89 Loss 0.0003357529058121145\n",
      "Epoch 1 Batch_Num 90 Loss 0.0014538750983774662\n",
      "Epoch 1 Batch_Num 91 Loss 0.0017731174593791366\n",
      "Epoch 1 Batch_Num 92 Loss 0.008668864145874977\n",
      "Epoch 1 Batch_Num 93 Loss 0.006768449209630489\n",
      "Epoch 1 Batch_Num 94 Loss 0.00458100438117981\n",
      "Epoch 1 Batch_Num 95 Loss 0.003049237187951803\n",
      "Epoch 1 Batch_Num 96 Loss 0.005825335625559092\n",
      "Epoch 1 Batch_Num 97 Loss 0.0008530416525900364\n",
      "Epoch 1 Batch_Num 98 Loss 0.004418978933244944\n",
      "Epoch 1 Batch_Num 99 Loss 0.0099759791046381\n",
      "Epoch 1 Batch_Num 100 Loss 0.0026382864452898502\n",
      "Epoch 1 Batch_Num 101 Loss 0.001353391446173191\n",
      "Epoch 1 Batch_Num 102 Loss 0.003332727123051882\n",
      "Epoch 1 Batch_Num 103 Loss 0.012265173718333244\n",
      "Epoch 1 Batch_Num 104 Loss 0.008814854547381401\n",
      "Epoch 1 Batch_Num 105 Loss 0.0032455376349389553\n",
      "Epoch 1 Batch_Num 106 Loss 0.0023149647749960423\n",
      "Epoch 1 Batch_Num 107 Loss 0.008775532245635986\n",
      "Epoch 1 Batch_Num 108 Loss 0.0018321732059121132\n",
      "Epoch 1 Batch_Num 109 Loss 0.005145050585269928\n",
      "Epoch 1 Batch_Num 110 Loss 0.012857255525887012\n",
      "Epoch 1 Batch_Num 111 Loss 0.004137719515711069\n",
      "Epoch 1 Batch_Num 112 Loss 0.007036974187940359\n",
      "Epoch 1 Batch_Num 113 Loss 0.009613381698727608\n",
      "Epoch 1 Batch_Num 114 Loss 0.00402835151180625\n",
      "Epoch 1 Batch_Num 115 Loss 0.003335140645503998\n",
      "Epoch 1 Batch_Num 116 Loss 0.0038341667968779802\n",
      "Epoch 1 Batch_Num 117 Loss 0.005690088029950857\n",
      "Epoch 1 Batch_Num 118 Loss 0.0017863918328657746\n",
      "Epoch 1 Batch_Num 119 Loss 0.008175283670425415\n",
      "Epoch 1 Batch_Num 120 Loss 0.007415974047034979\n",
      "Epoch 1 Batch_Num 121 Loss 0.009174598380923271\n",
      "Epoch 1 Batch_Num 122 Loss 0.003952636383473873\n",
      "Epoch 1 Batch_Num 123 Loss 0.007964115589857101\n",
      "Epoch 1 Batch_Num 124 Loss 0.004156759940087795\n",
      "Epoch 1 Batch_Num 125 Loss 0.0029917617794126272\n",
      "Epoch 1 Batch_Num 126 Loss 0.003572695655748248\n",
      "Epoch 1 Batch_Num 127 Loss 0.004097982309758663\n",
      "Epoch 1 Batch_Num 128 Loss 0.0033781412057578564\n",
      "Epoch 1 Batch_Num 129 Loss 0.00141090527176857\n",
      "Epoch 1 Batch_Num 130 Loss 0.005901250056922436\n",
      "Epoch 1 Batch_Num 131 Loss 0.003787239547818899\n",
      "Epoch 1 Batch_Num 132 Loss 0.0018727447604760528\n",
      "Epoch 1 Batch_Num 133 Loss 0.004937625490128994\n",
      "Epoch 1 Batch_Num 134 Loss 0.009431010112166405\n",
      "Epoch 1 Batch_Num 135 Loss 0.007151986006647348\n",
      "Epoch 1 Batch_Num 136 Loss 0.007550098933279514\n",
      "Epoch 1 Batch_Num 137 Loss 0.009249014779925346\n",
      "Epoch 1 Batch_Num 138 Loss 0.006009931210428476\n",
      "Epoch 1 Batch_Num 139 Loss 0.0029704789631068707\n",
      "Epoch 1 Batch_Num 140 Loss 0.0027171517722308636\n",
      "Epoch 1 Batch_Num 141 Loss 0.0019864507485181093\n",
      "Epoch 1 Batch_Num 142 Loss 0.00840457621961832\n",
      "Epoch 1 Batch_Num 143 Loss 0.006826377473771572\n",
      "Epoch 1 Batch_Num 144 Loss 0.008512306958436966\n",
      "Epoch 1 Batch_Num 145 Loss 0.003147457493469119\n",
      "Epoch 1 Batch_Num 146 Loss 0.0062075466848909855\n",
      "Epoch 1 Batch_Num 147 Loss 0.00730751221999526\n",
      "Epoch 1 Batch_Num 148 Loss 0.0012173503637313843\n",
      "Epoch 1 Batch_Num 149 Loss 0.002092947019264102\n",
      "Epoch 1 Batch_Num 150 Loss 0.0052287383005023\n",
      "Epoch 1 Batch_Num 151 Loss 0.0042380522936582565\n",
      "Epoch 1 Batch_Num 152 Loss 0.0008566564065404236\n",
      "Epoch 1 Batch_Num 153 Loss 0.014196664094924927\n",
      "Epoch 1 Batch_Num 154 Loss 0.001150957541540265\n",
      "Epoch 1 Batch_Num 155 Loss 0.00569887412711978\n",
      "Epoch 1 Batch_Num 156 Loss 0.0045456779189407825\n",
      "Epoch 1 Batch_Num 157 Loss 0.007359088398516178\n",
      "Epoch 1 Batch_Num 158 Loss 0.00554086035117507\n",
      "Epoch 1 Batch_Num 159 Loss 0.006844832096248865\n",
      "Epoch 1 Batch_Num 160 Loss 0.002796738175675273\n",
      "Epoch 1 Batch_Num 161 Loss 0.009726339019834995\n",
      "Epoch 1 Batch_Num 162 Loss 0.005405391566455364\n",
      "Epoch 1 Batch_Num 163 Loss 0.004927809350192547\n",
      "Epoch 1 Batch_Num 164 Loss 0.00729625066742301\n",
      "Epoch 1 Batch_Num 165 Loss 0.003886935766786337\n",
      "Epoch 1 Batch_Num 166 Loss 0.01037283893674612\n",
      "Epoch 1 Batch_Num 167 Loss 0.002247527241706848\n",
      "Epoch 1 Batch_Num 168 Loss 0.005638855043798685\n",
      "Epoch 1 Batch_Num 169 Loss 0.004740585573017597\n",
      "Epoch 1 Batch_Num 170 Loss 0.006610563490539789\n",
      "Epoch 1 Batch_Num 171 Loss 0.002673643408343196\n",
      "Epoch 1 Batch_Num 172 Loss 0.003420337336137891\n",
      "Epoch 1 Batch_Num 173 Loss 0.004959470592439175\n",
      "Epoch 1 Batch_Num 174 Loss 0.0009759255917742848\n",
      "Epoch 1 Batch_Num 175 Loss 0.002651812043040991\n",
      "Epoch 1 Batch_Num 176 Loss 0.0019470055121928453\n",
      "Epoch 1 Batch_Num 177 Loss 0.010324850678443909\n",
      "Epoch 1 Batch_Num 178 Loss 0.0073112077079713345\n",
      "Epoch 1 Batch_Num 179 Loss 0.0074363043531775475\n",
      "Epoch 1 Batch_Num 180 Loss 0.008961101993918419\n",
      "Epoch 1 Batch_Num 181 Loss 0.0048116957768797874\n",
      "Epoch 1 Batch_Num 182 Loss 0.002370462054386735\n",
      "Epoch 1 Batch_Num 183 Loss 0.0027573774568736553\n",
      "Epoch 1 Batch_Num 184 Loss 0.006429006811231375\n",
      "Epoch 1 Batch_Num 185 Loss 0.005496134515851736\n",
      "Epoch 1 Batch_Num 186 Loss 0.006770202424377203\n",
      "Epoch 1 Batch_Num 187 Loss 0.005097869783639908\n",
      "Epoch 1 Batch_Num 188 Loss 0.002574261510744691\n",
      "Epoch 1 Batch_Num 189 Loss 0.0014590277569368482\n",
      "Epoch 1 Batch_Num 190 Loss 0.0020297537557780743\n",
      "Epoch 1 Batch_Num 191 Loss 0.005748281721025705\n",
      "Epoch 1 Batch_Num 192 Loss 0.0055809542536735535\n",
      "Epoch 1 Batch_Num 193 Loss 0.0037526339292526245\n",
      "Epoch 1 Batch_Num 194 Loss 0.002087146043777466\n",
      "Epoch 1 Batch_Num 195 Loss 0.0024423671420663595\n",
      "Epoch 1 Batch_Num 196 Loss 0.01344494242221117\n",
      "Epoch 1 Batch_Num 197 Loss 0.005067579448223114\n",
      "Epoch 1 Batch_Num 198 Loss 0.0051423790864646435\n",
      "Epoch 1 Batch_Num 199 Loss 0.0049580736085772514\n",
      "Epoch 1 Batch_Num 200 Loss 0.004292726516723633\n",
      "Epoch 1 Batch_Num 201 Loss 0.0018504156032577157\n",
      "Epoch 1 Batch_Num 202 Loss 0.004175850190222263\n",
      "Epoch 1 Batch_Num 203 Loss 0.0026395227760076523\n",
      "Epoch 1 Batch_Num 204 Loss 0.0024872557260096073\n",
      "Epoch 1 Batch_Num 205 Loss 0.00424070144072175\n",
      "Epoch 1 Batch_Num 206 Loss 0.01077914796769619\n",
      "Epoch 1 Batch_Num 207 Loss 0.006342784501612186\n",
      "Epoch 1 Batch_Num 208 Loss 0.0032746619544923306\n",
      "Epoch 1 Batch_Num 209 Loss 0.002695238683372736\n",
      "Epoch 1 Batch_Num 210 Loss 0.0032946437131613493\n",
      "Epoch 1 Batch_Num 211 Loss 0.005204378627240658\n",
      "Epoch 1 Batch_Num 212 Loss 0.0011203658068552613\n",
      "Epoch 1 Batch_Num 213 Loss 0.003992943093180656\n",
      "Epoch 1 Batch_Num 214 Loss 0.0027854801155626774\n",
      "Epoch 1 Batch_Num 215 Loss 0.0029296078719198704\n",
      "Epoch 1 Batch_Num 216 Loss 0.004484556615352631\n",
      "Epoch 1 Batch_Num 217 Loss 0.00447242334485054\n",
      "Epoch 1 Batch_Num 218 Loss 0.0022760392166674137\n",
      "Epoch 1 Batch_Num 219 Loss 0.002199894981458783\n",
      "Epoch 1 Batch_Num 220 Loss 0.001549412147141993\n",
      "Epoch 1 Batch_Num 221 Loss 0.0042503587901592255\n",
      "Epoch 1 Batch_Num 222 Loss 0.004580132197588682\n",
      "Epoch 1 Batch_Num 223 Loss 0.0024093729443848133\n",
      "Epoch 1 Batch_Num 224 Loss 0.0022743460722267628\n",
      "Epoch 1 Batch_Num 225 Loss 0.004824713803827763\n",
      "Epoch 1 Batch_Num 226 Loss 0.0017820040229707956\n",
      "Epoch 1 Batch_Num 227 Loss 4.987576903658919e-05\n",
      "Epoch 1 Batch_Num 228 Loss 0.0006241975352168083\n",
      "Epoch 1 Batch_Num 229 Loss 0.0005549831548705697\n",
      "Epoch 1 Batch_Num 230 Loss 2.2175201593199745e-05\n",
      "Epoch 1 Batch_Num 231 Loss 0.000607205496635288\n",
      "Epoch 1 Batch_Num 232 Loss 0.002987136598676443\n",
      "Epoch 1 Batch_Num 233 Loss 0.009043069556355476\n",
      "Epoch 1 Batch_Num 234 Loss 0.005957498215138912\n",
      "Epoch 2 Batch_Num 0 Loss 0.006268533878028393\n",
      "Epoch 2 Batch_Num 1 Loss 0.0033468231558799744\n",
      "Epoch 2 Batch_Num 2 Loss 0.0026285620406270027\n",
      "Epoch 2 Batch_Num 3 Loss 0.010445374995470047\n",
      "Epoch 2 Batch_Num 4 Loss 0.008102155290544033\n",
      "Epoch 2 Batch_Num 5 Loss 0.007550881709903479\n",
      "Epoch 2 Batch_Num 6 Loss 0.007601181976497173\n",
      "Epoch 2 Batch_Num 7 Loss 0.0017509183380752802\n",
      "Epoch 2 Batch_Num 8 Loss 0.003122317139059305\n",
      "Epoch 2 Batch_Num 9 Loss 0.005655443295836449\n",
      "Epoch 2 Batch_Num 10 Loss 0.006976974196732044\n",
      "Epoch 2 Batch_Num 11 Loss 0.006592231802642345\n",
      "Epoch 2 Batch_Num 12 Loss 0.0032048069406300783\n",
      "Epoch 2 Batch_Num 13 Loss 0.005082139279693365\n",
      "Epoch 2 Batch_Num 14 Loss 0.0092148557305336\n",
      "Epoch 2 Batch_Num 15 Loss 0.002439743373543024\n",
      "Epoch 2 Batch_Num 16 Loss 0.0035872659645974636\n",
      "Epoch 2 Batch_Num 17 Loss 0.004670657217502594\n",
      "Epoch 2 Batch_Num 18 Loss 0.004577858839184046\n",
      "Epoch 2 Batch_Num 19 Loss 0.0034022927284240723\n",
      "Epoch 2 Batch_Num 20 Loss 0.004997792653739452\n",
      "Epoch 2 Batch_Num 21 Loss 0.006170745473355055\n",
      "Epoch 2 Batch_Num 22 Loss 0.008134106174111366\n",
      "Epoch 2 Batch_Num 23 Loss 0.010604342445731163\n",
      "Epoch 2 Batch_Num 24 Loss 0.001975241582840681\n",
      "Epoch 2 Batch_Num 25 Loss 0.0019653080962598324\n",
      "Epoch 2 Batch_Num 26 Loss 0.0022876712027937174\n",
      "Epoch 2 Batch_Num 27 Loss 0.0027871797792613506\n",
      "Epoch 2 Batch_Num 28 Loss 0.007326188031584024\n",
      "Epoch 2 Batch_Num 29 Loss 0.002555133542045951\n",
      "Epoch 2 Batch_Num 30 Loss 0.006363585591316223\n",
      "Epoch 2 Batch_Num 31 Loss 0.008079109713435173\n",
      "Epoch 2 Batch_Num 32 Loss 0.009528852067887783\n",
      "Epoch 2 Batch_Num 33 Loss 0.001964104361832142\n",
      "Epoch 2 Batch_Num 34 Loss 0.00706197926774621\n",
      "Epoch 2 Batch_Num 35 Loss 0.002513411222025752\n",
      "Epoch 2 Batch_Num 36 Loss 0.0034037542063742876\n",
      "Epoch 2 Batch_Num 37 Loss 0.009239578619599342\n",
      "Epoch 2 Batch_Num 38 Loss 0.003961241338402033\n",
      "Epoch 2 Batch_Num 39 Loss 0.0020368569530546665\n",
      "Epoch 2 Batch_Num 40 Loss 0.0057364897802472115\n",
      "Epoch 2 Batch_Num 41 Loss 0.0020193560048937798\n",
      "Epoch 2 Batch_Num 42 Loss 0.012758140452206135\n",
      "Epoch 2 Batch_Num 43 Loss 0.004052025265991688\n",
      "Epoch 2 Batch_Num 44 Loss 0.0015543334884569049\n",
      "Epoch 2 Batch_Num 45 Loss 0.007597516290843487\n",
      "Epoch 2 Batch_Num 46 Loss 0.008005867712199688\n",
      "Epoch 2 Batch_Num 47 Loss 0.0017516264924779534\n",
      "Epoch 2 Batch_Num 48 Loss 0.0028196244966238737\n",
      "Epoch 2 Batch_Num 49 Loss 0.0063173407688736916\n",
      "Epoch 2 Batch_Num 50 Loss 0.0036076910328119993\n",
      "Epoch 2 Batch_Num 51 Loss 0.006248212419450283\n",
      "Epoch 2 Batch_Num 52 Loss 0.006371199153363705\n",
      "Epoch 2 Batch_Num 53 Loss 0.005356078036129475\n",
      "Epoch 2 Batch_Num 54 Loss 0.006183087360113859\n",
      "Epoch 2 Batch_Num 55 Loss 0.00433582765981555\n",
      "Epoch 2 Batch_Num 56 Loss 0.004497656133025885\n",
      "Epoch 2 Batch_Num 57 Loss 0.009124774485826492\n",
      "Epoch 2 Batch_Num 58 Loss 0.0009825039887800813\n",
      "Epoch 2 Batch_Num 59 Loss 0.009018841199576855\n",
      "Epoch 2 Batch_Num 60 Loss 0.005782910156995058\n",
      "Epoch 2 Batch_Num 61 Loss 0.005565009545534849\n",
      "Epoch 2 Batch_Num 62 Loss 0.0051870280876755714\n",
      "Epoch 2 Batch_Num 63 Loss 0.005370778031647205\n",
      "Epoch 2 Batch_Num 64 Loss 0.0018747157882899046\n",
      "Epoch 2 Batch_Num 65 Loss 0.011948752216994762\n",
      "Epoch 2 Batch_Num 66 Loss 0.005302011966705322\n",
      "Epoch 2 Batch_Num 67 Loss 0.0048867398872971535\n",
      "Epoch 2 Batch_Num 68 Loss 0.003259517252445221\n",
      "Epoch 2 Batch_Num 69 Loss 0.0030076163820922375\n",
      "Epoch 2 Batch_Num 70 Loss 0.007692644838243723\n",
      "Epoch 2 Batch_Num 71 Loss 0.003860571887344122\n",
      "Epoch 2 Batch_Num 72 Loss 0.001332342391833663\n",
      "Epoch 2 Batch_Num 73 Loss 0.0028887127991765738\n",
      "Epoch 2 Batch_Num 74 Loss 0.0008085538865998387\n",
      "Epoch 2 Batch_Num 75 Loss 0.00813900027424097\n",
      "Epoch 2 Batch_Num 76 Loss 0.001915248460136354\n",
      "Epoch 2 Batch_Num 77 Loss 0.004803603049367666\n",
      "Epoch 2 Batch_Num 78 Loss 0.004820667207241058\n",
      "Epoch 2 Batch_Num 79 Loss 0.0029033322352916002\n",
      "Epoch 2 Batch_Num 80 Loss 0.0012885674368590117\n",
      "Epoch 2 Batch_Num 81 Loss 0.0042264885269105434\n",
      "Epoch 2 Batch_Num 82 Loss 0.005042491015046835\n",
      "Epoch 2 Batch_Num 83 Loss 0.0007089952123351395\n",
      "Epoch 2 Batch_Num 84 Loss 0.006440100725740194\n",
      "Epoch 2 Batch_Num 85 Loss 0.005539917387068272\n",
      "Epoch 2 Batch_Num 86 Loss 0.002076000440865755\n",
      "Epoch 2 Batch_Num 87 Loss 0.0012644449016079307\n",
      "Epoch 2 Batch_Num 88 Loss 0.005179902538657188\n",
      "Epoch 2 Batch_Num 89 Loss 0.0014405352994799614\n",
      "Epoch 2 Batch_Num 90 Loss 0.0024893509689718485\n",
      "Epoch 2 Batch_Num 91 Loss 0.0014199999859556556\n",
      "Epoch 2 Batch_Num 92 Loss 0.005615544505417347\n",
      "Epoch 2 Batch_Num 93 Loss 0.005022113677114248\n",
      "Epoch 2 Batch_Num 94 Loss 0.0021326919086277485\n",
      "Epoch 2 Batch_Num 95 Loss 0.0034084997605532408\n",
      "Epoch 2 Batch_Num 96 Loss 0.008690854534506798\n",
      "Epoch 2 Batch_Num 97 Loss 0.005307340063154697\n",
      "Epoch 2 Batch_Num 98 Loss 0.0045394147746264935\n",
      "Epoch 2 Batch_Num 99 Loss 0.007286085281521082\n",
      "Epoch 2 Batch_Num 100 Loss 0.004349695984274149\n",
      "Epoch 2 Batch_Num 101 Loss 0.004990309476852417\n",
      "Epoch 2 Batch_Num 102 Loss 0.0012813014909625053\n",
      "Epoch 2 Batch_Num 103 Loss 0.014682406559586525\n",
      "Epoch 2 Batch_Num 104 Loss 0.01159681286662817\n",
      "Epoch 2 Batch_Num 105 Loss 0.003842276753857732\n",
      "Epoch 2 Batch_Num 106 Loss 0.007074006833136082\n",
      "Epoch 2 Batch_Num 107 Loss 0.006115637253969908\n",
      "Epoch 2 Batch_Num 108 Loss 0.0018674975726753473\n",
      "Epoch 2 Batch_Num 109 Loss 0.008522965013980865\n",
      "Epoch 2 Batch_Num 110 Loss 0.010010454803705215\n",
      "Epoch 2 Batch_Num 111 Loss 0.0026006833650171757\n",
      "Epoch 2 Batch_Num 112 Loss 0.003714118618518114\n",
      "Epoch 2 Batch_Num 113 Loss 0.0056991479359567165\n",
      "Epoch 2 Batch_Num 114 Loss 0.009706820361316204\n",
      "Epoch 2 Batch_Num 115 Loss 0.004224973730742931\n",
      "Epoch 2 Batch_Num 116 Loss 0.004924280568957329\n",
      "Epoch 2 Batch_Num 117 Loss 0.007766242139041424\n",
      "Epoch 2 Batch_Num 118 Loss 0.0014480318641290069\n",
      "Epoch 2 Batch_Num 119 Loss 0.0056445905938744545\n",
      "Epoch 2 Batch_Num 120 Loss 0.0019525913521647453\n",
      "Epoch 2 Batch_Num 121 Loss 0.005886607803404331\n",
      "Epoch 2 Batch_Num 122 Loss 0.00672359112650156\n",
      "Epoch 2 Batch_Num 123 Loss 0.010081379674375057\n",
      "Epoch 2 Batch_Num 124 Loss 0.001476644305512309\n",
      "Epoch 2 Batch_Num 125 Loss 0.004209678154438734\n",
      "Epoch 2 Batch_Num 126 Loss 0.005230890121310949\n",
      "Epoch 2 Batch_Num 127 Loss 0.005548700224608183\n",
      "Epoch 2 Batch_Num 128 Loss 0.003999361302703619\n",
      "Epoch 2 Batch_Num 129 Loss 0.003310754429548979\n",
      "Epoch 2 Batch_Num 130 Loss 0.0051522813737392426\n",
      "Epoch 2 Batch_Num 131 Loss 0.0033178902231156826\n",
      "Epoch 2 Batch_Num 132 Loss 0.005981958471238613\n",
      "Epoch 2 Batch_Num 133 Loss 0.002486505312845111\n",
      "Epoch 2 Batch_Num 134 Loss 0.002248521661385894\n",
      "Epoch 2 Batch_Num 135 Loss 0.007425656076520681\n",
      "Epoch 2 Batch_Num 136 Loss 0.006914754398167133\n",
      "Epoch 2 Batch_Num 137 Loss 0.005474310368299484\n",
      "Epoch 2 Batch_Num 138 Loss 0.010255947709083557\n",
      "Epoch 2 Batch_Num 139 Loss 0.004168235696852207\n",
      "Epoch 2 Batch_Num 140 Loss 0.004195849411189556\n",
      "Epoch 2 Batch_Num 141 Loss 0.0014933380298316479\n",
      "Epoch 2 Batch_Num 142 Loss 0.005338606424629688\n",
      "Epoch 2 Batch_Num 143 Loss 0.0022320791613310575\n",
      "Epoch 2 Batch_Num 144 Loss 0.013278165832161903\n",
      "Epoch 2 Batch_Num 145 Loss 0.006814463529735804\n",
      "Epoch 2 Batch_Num 146 Loss 0.009370624087750912\n",
      "Epoch 2 Batch_Num 147 Loss 0.005620361305773258\n",
      "Epoch 2 Batch_Num 148 Loss 0.0009304647101089358\n",
      "Epoch 2 Batch_Num 149 Loss 0.006610211916267872\n",
      "Epoch 2 Batch_Num 150 Loss 0.019467804580926895\n",
      "Epoch 2 Batch_Num 151 Loss 0.0035285588819533587\n",
      "Epoch 2 Batch_Num 152 Loss 0.0025341007858514786\n",
      "Epoch 2 Batch_Num 153 Loss 0.012761855497956276\n",
      "Epoch 2 Batch_Num 154 Loss 0.004399510100483894\n",
      "Epoch 2 Batch_Num 155 Loss 0.011590085923671722\n",
      "Epoch 2 Batch_Num 156 Loss 0.009547639638185501\n",
      "Epoch 2 Batch_Num 157 Loss 0.009154888801276684\n",
      "Epoch 2 Batch_Num 158 Loss 0.008493234403431416\n",
      "Epoch 2 Batch_Num 159 Loss 0.011186652816832066\n",
      "Epoch 2 Batch_Num 160 Loss 0.004161205142736435\n",
      "Epoch 2 Batch_Num 161 Loss 0.014007195830345154\n",
      "Epoch 2 Batch_Num 162 Loss 0.004913136828690767\n",
      "Epoch 2 Batch_Num 163 Loss 0.003873693523928523\n",
      "Epoch 2 Batch_Num 164 Loss 0.002544121816754341\n",
      "Epoch 2 Batch_Num 165 Loss 0.005293559283018112\n",
      "Epoch 2 Batch_Num 166 Loss 0.003121796064078808\n",
      "Epoch 2 Batch_Num 167 Loss 0.0035253427922725677\n",
      "Epoch 2 Batch_Num 168 Loss 0.010659430176019669\n",
      "Epoch 2 Batch_Num 169 Loss 0.005134506616741419\n",
      "Epoch 2 Batch_Num 170 Loss 0.004899069666862488\n",
      "Epoch 2 Batch_Num 171 Loss 0.00620579207316041\n",
      "Epoch 2 Batch_Num 172 Loss 0.005594749003648758\n",
      "Epoch 2 Batch_Num 173 Loss 0.006795936264097691\n",
      "Epoch 2 Batch_Num 174 Loss 0.0022725402377545834\n",
      "Epoch 2 Batch_Num 175 Loss 0.0013751891674473882\n",
      "Epoch 2 Batch_Num 176 Loss 0.0023631826043128967\n",
      "Epoch 2 Batch_Num 177 Loss 0.008917453698813915\n",
      "Epoch 2 Batch_Num 178 Loss 0.0034873057156801224\n",
      "Epoch 2 Batch_Num 179 Loss 0.003945232834666967\n",
      "Epoch 2 Batch_Num 180 Loss 0.006138825323432684\n",
      "Epoch 2 Batch_Num 181 Loss 0.005079618655145168\n",
      "Epoch 2 Batch_Num 182 Loss 0.0024085184559226036\n",
      "Epoch 2 Batch_Num 183 Loss 0.0006884295726194978\n",
      "Epoch 2 Batch_Num 184 Loss 0.008099855855107307\n",
      "Epoch 2 Batch_Num 185 Loss 0.005886843893676996\n",
      "Epoch 2 Batch_Num 186 Loss 0.003988512326031923\n",
      "Epoch 2 Batch_Num 187 Loss 0.0014100137632340193\n",
      "Epoch 2 Batch_Num 188 Loss 0.00630910461768508\n",
      "Epoch 2 Batch_Num 189 Loss 0.0027262114454060793\n",
      "Epoch 2 Batch_Num 190 Loss 0.004013194236904383\n",
      "Epoch 2 Batch_Num 191 Loss 0.00787701178342104\n",
      "Epoch 2 Batch_Num 192 Loss 0.011724269017577171\n",
      "Epoch 2 Batch_Num 193 Loss 0.003925973549485207\n",
      "Epoch 2 Batch_Num 194 Loss 0.013706056401133537\n",
      "Epoch 2 Batch_Num 195 Loss 0.0006647006375715137\n",
      "Epoch 2 Batch_Num 196 Loss 0.006335797253996134\n",
      "Epoch 2 Batch_Num 197 Loss 0.005757718347012997\n",
      "Epoch 2 Batch_Num 198 Loss 0.0020987975876778364\n",
      "Epoch 2 Batch_Num 199 Loss 0.005624957848340273\n",
      "Epoch 2 Batch_Num 200 Loss 0.003227452514693141\n",
      "Epoch 2 Batch_Num 201 Loss 0.008373410440981388\n",
      "Epoch 2 Batch_Num 202 Loss 0.011724334210157394\n",
      "Epoch 2 Batch_Num 203 Loss 0.001844563870690763\n",
      "Epoch 2 Batch_Num 204 Loss 0.005117807071655989\n",
      "Epoch 2 Batch_Num 205 Loss 0.0016235715011134744\n",
      "Epoch 2 Batch_Num 206 Loss 0.006301644258201122\n",
      "Epoch 2 Batch_Num 207 Loss 0.008384281769394875\n",
      "Epoch 2 Batch_Num 208 Loss 0.002312171272933483\n",
      "Epoch 2 Batch_Num 209 Loss 0.004419660661369562\n",
      "Epoch 2 Batch_Num 210 Loss 0.0035082853864878416\n",
      "Epoch 2 Batch_Num 211 Loss 0.0021340535022318363\n",
      "Epoch 2 Batch_Num 212 Loss 0.0017932879272848368\n",
      "Epoch 2 Batch_Num 213 Loss 0.010712861083447933\n",
      "Epoch 2 Batch_Num 214 Loss 0.005246495828032494\n",
      "Epoch 2 Batch_Num 215 Loss 0.0015508963260799646\n",
      "Epoch 2 Batch_Num 216 Loss 0.0035054709296673536\n",
      "Epoch 2 Batch_Num 217 Loss 0.0028574850875884295\n",
      "Epoch 2 Batch_Num 218 Loss 0.0024777024518698454\n",
      "Epoch 2 Batch_Num 219 Loss 0.003052707063034177\n",
      "Epoch 2 Batch_Num 220 Loss 0.0036478948313742876\n",
      "Epoch 2 Batch_Num 221 Loss 0.012855741195380688\n",
      "Epoch 2 Batch_Num 222 Loss 0.001854854985140264\n",
      "Epoch 2 Batch_Num 223 Loss 0.0007245039450936019\n",
      "Epoch 2 Batch_Num 224 Loss 0.002759090391919017\n",
      "Epoch 2 Batch_Num 225 Loss 0.00238979235291481\n",
      "Epoch 2 Batch_Num 226 Loss 0.004587777424603701\n",
      "Epoch 2 Batch_Num 227 Loss 0.0008208631770685315\n",
      "Epoch 2 Batch_Num 228 Loss 0.0005418608197942376\n",
      "Epoch 2 Batch_Num 229 Loss 0.0029973878990858793\n",
      "Epoch 2 Batch_Num 230 Loss 0.004504648502916098\n",
      "Epoch 2 Batch_Num 231 Loss 0.0006310330936685205\n",
      "Epoch 2 Batch_Num 232 Loss 0.001340369926765561\n",
      "Epoch 2 Batch_Num 233 Loss 0.008824551478028297\n",
      "Epoch 2 Batch_Num 234 Loss 0.0044404142536222935\n",
      "Epoch 3 Batch_Num 0 Loss 0.006554276682436466\n",
      "Epoch 3 Batch_Num 1 Loss 0.00549107464030385\n",
      "Epoch 3 Batch_Num 2 Loss 0.002277747495099902\n",
      "Epoch 3 Batch_Num 3 Loss 0.007648838218301535\n",
      "Epoch 3 Batch_Num 4 Loss 0.0017318783793598413\n",
      "Epoch 3 Batch_Num 5 Loss 0.0090885479003191\n",
      "Epoch 3 Batch_Num 6 Loss 0.00578926969319582\n",
      "Epoch 3 Batch_Num 7 Loss 0.00287715345621109\n",
      "Epoch 3 Batch_Num 8 Loss 0.0014585619792342186\n",
      "Epoch 3 Batch_Num 9 Loss 0.003544899169355631\n",
      "Epoch 3 Batch_Num 10 Loss 0.009041368961334229\n",
      "Epoch 3 Batch_Num 11 Loss 0.008715379051864147\n",
      "Epoch 3 Batch_Num 12 Loss 0.0012114355340600014\n",
      "Epoch 3 Batch_Num 13 Loss 0.002465111669152975\n",
      "Epoch 3 Batch_Num 14 Loss 0.007032214663922787\n",
      "Epoch 3 Batch_Num 15 Loss 0.0017251267563551664\n",
      "Epoch 3 Batch_Num 16 Loss 0.0030429239850491285\n",
      "Epoch 3 Batch_Num 17 Loss 0.0038721133023500443\n",
      "Epoch 3 Batch_Num 18 Loss 0.0015023833839222789\n",
      "Epoch 3 Batch_Num 19 Loss 0.006053004413843155\n",
      "Epoch 3 Batch_Num 20 Loss 0.00413466477766633\n",
      "Epoch 3 Batch_Num 21 Loss 0.001145619316957891\n",
      "Epoch 3 Batch_Num 22 Loss 0.00358753209002316\n",
      "Epoch 3 Batch_Num 23 Loss 0.0033611771650612354\n",
      "Epoch 3 Batch_Num 24 Loss 0.0022655439097434282\n",
      "Epoch 3 Batch_Num 25 Loss 0.0010589517187327147\n",
      "Epoch 3 Batch_Num 26 Loss 0.003294882131740451\n",
      "Epoch 3 Batch_Num 27 Loss 0.005383866373449564\n",
      "Epoch 3 Batch_Num 28 Loss 0.006194340996444225\n",
      "Epoch 3 Batch_Num 29 Loss 0.0011581381550058722\n",
      "Epoch 3 Batch_Num 30 Loss 0.004014770034700632\n",
      "Epoch 3 Batch_Num 31 Loss 0.0007867094245739281\n",
      "Epoch 3 Batch_Num 32 Loss 0.0035220098216086626\n",
      "Epoch 3 Batch_Num 33 Loss 0.0021890955977141857\n",
      "Epoch 3 Batch_Num 34 Loss 0.012983033433556557\n",
      "Epoch 3 Batch_Num 35 Loss 0.002259217668324709\n",
      "Epoch 3 Batch_Num 36 Loss 0.0028519118204712868\n",
      "Epoch 3 Batch_Num 37 Loss 0.0006188674014993012\n",
      "Epoch 3 Batch_Num 38 Loss 0.0011087056482210755\n",
      "Epoch 3 Batch_Num 39 Loss 0.004600174259394407\n",
      "Epoch 3 Batch_Num 40 Loss 0.006257650908082724\n",
      "Epoch 3 Batch_Num 41 Loss 0.0003412085061427206\n",
      "Epoch 3 Batch_Num 42 Loss 0.00795092061161995\n",
      "Epoch 3 Batch_Num 43 Loss 0.003288928885012865\n",
      "Epoch 3 Batch_Num 44 Loss 0.0008179129799827933\n",
      "Epoch 3 Batch_Num 45 Loss 0.002457153517752886\n",
      "Epoch 3 Batch_Num 46 Loss 0.0028378304559737444\n",
      "Epoch 3 Batch_Num 47 Loss 0.004017985425889492\n",
      "Epoch 3 Batch_Num 48 Loss 0.004890593234449625\n",
      "Epoch 3 Batch_Num 49 Loss 0.003378258552402258\n",
      "Epoch 3 Batch_Num 50 Loss 0.0012872203951701522\n",
      "Epoch 3 Batch_Num 51 Loss 0.011735664680600166\n",
      "Epoch 3 Batch_Num 52 Loss 0.006298901047557592\n",
      "Epoch 3 Batch_Num 53 Loss 0.0026230383664369583\n",
      "Epoch 3 Batch_Num 54 Loss 0.01328320987522602\n",
      "Epoch 3 Batch_Num 55 Loss 0.0020943942945450544\n",
      "Epoch 3 Batch_Num 56 Loss 0.004085822496563196\n",
      "Epoch 3 Batch_Num 57 Loss 0.004514562897384167\n",
      "Epoch 3 Batch_Num 58 Loss 0.0007585083367303014\n",
      "Epoch 3 Batch_Num 59 Loss 0.004500093869864941\n",
      "Epoch 3 Batch_Num 60 Loss 0.004602071363478899\n",
      "Epoch 3 Batch_Num 61 Loss 0.006672483868896961\n",
      "Epoch 3 Batch_Num 62 Loss 0.006917905993759632\n",
      "Epoch 3 Batch_Num 63 Loss 0.0029376603197306395\n",
      "Epoch 3 Batch_Num 64 Loss 0.005070899613201618\n",
      "Epoch 3 Batch_Num 65 Loss 0.007472512777894735\n",
      "Epoch 3 Batch_Num 66 Loss 0.00773278996348381\n",
      "Epoch 3 Batch_Num 67 Loss 0.0042179180309176445\n",
      "Epoch 3 Batch_Num 68 Loss 0.0028300718404352665\n",
      "Epoch 3 Batch_Num 69 Loss 0.008682554587721825\n",
      "Epoch 3 Batch_Num 70 Loss 0.0025258571840822697\n",
      "Epoch 3 Batch_Num 71 Loss 0.003447646275162697\n",
      "Epoch 3 Batch_Num 72 Loss 0.0022201002575457096\n",
      "Epoch 3 Batch_Num 73 Loss 0.00030099533614702523\n",
      "Epoch 3 Batch_Num 74 Loss 0.0030081188306212425\n",
      "Epoch 3 Batch_Num 75 Loss 0.0033900022972375154\n",
      "Epoch 3 Batch_Num 76 Loss 0.001944736810401082\n",
      "Epoch 3 Batch_Num 77 Loss 0.006800471339374781\n",
      "Epoch 3 Batch_Num 78 Loss 0.009658603928983212\n",
      "Epoch 3 Batch_Num 79 Loss 0.003859433112666011\n",
      "Epoch 3 Batch_Num 80 Loss 0.0013993976172059774\n",
      "Epoch 3 Batch_Num 81 Loss 0.002340965438634157\n",
      "Epoch 3 Batch_Num 82 Loss 0.003328384831547737\n",
      "Epoch 3 Batch_Num 83 Loss 0.0019829748198390007\n",
      "Epoch 3 Batch_Num 84 Loss 0.005321649368852377\n",
      "Epoch 3 Batch_Num 85 Loss 0.0036539770662784576\n",
      "Epoch 3 Batch_Num 86 Loss 0.002544465707615018\n",
      "Epoch 3 Batch_Num 87 Loss 0.001980929635465145\n",
      "Epoch 3 Batch_Num 88 Loss 0.0035092767793685198\n",
      "Epoch 3 Batch_Num 89 Loss 0.0006869110511615872\n",
      "Epoch 3 Batch_Num 90 Loss 0.0034774120431393385\n",
      "Epoch 3 Batch_Num 91 Loss 0.0011072983033955097\n",
      "Epoch 3 Batch_Num 92 Loss 0.004251527599990368\n",
      "Epoch 3 Batch_Num 93 Loss 0.005457570310682058\n",
      "Epoch 3 Batch_Num 94 Loss 0.0015955343842506409\n",
      "Epoch 3 Batch_Num 95 Loss 0.0022309930063784122\n",
      "Epoch 3 Batch_Num 96 Loss 0.003922546282410622\n",
      "Epoch 3 Batch_Num 97 Loss 0.0020197657868266106\n",
      "Epoch 3 Batch_Num 98 Loss 0.0012474681716412306\n",
      "Epoch 3 Batch_Num 99 Loss 0.005587780382484198\n",
      "Epoch 3 Batch_Num 100 Loss 0.0023094196803867817\n",
      "Epoch 3 Batch_Num 101 Loss 0.0016061108326539397\n",
      "Epoch 3 Batch_Num 102 Loss 0.0010899112094193697\n",
      "Epoch 3 Batch_Num 103 Loss 0.01446667592972517\n",
      "Epoch 3 Batch_Num 104 Loss 0.005291891284286976\n",
      "Epoch 3 Batch_Num 105 Loss 0.0006547443917952478\n",
      "Epoch 3 Batch_Num 106 Loss 0.00260237162001431\n",
      "Epoch 3 Batch_Num 107 Loss 0.00628639105707407\n",
      "Epoch 3 Batch_Num 108 Loss 0.0019811918027698994\n",
      "Epoch 3 Batch_Num 109 Loss 0.0011554558295756578\n",
      "Epoch 3 Batch_Num 110 Loss 0.006459527648985386\n",
      "Epoch 3 Batch_Num 111 Loss 0.0038420483469963074\n",
      "Epoch 3 Batch_Num 112 Loss 0.004033808596432209\n",
      "Epoch 3 Batch_Num 113 Loss 0.004521824419498444\n",
      "Epoch 3 Batch_Num 114 Loss 0.009756255894899368\n",
      "Epoch 3 Batch_Num 115 Loss 0.0018793020863085985\n",
      "Epoch 3 Batch_Num 116 Loss 0.0009453592938371003\n",
      "Epoch 3 Batch_Num 117 Loss 0.004102427512407303\n",
      "Epoch 3 Batch_Num 118 Loss 0.00011873619951074943\n",
      "Epoch 3 Batch_Num 119 Loss 0.0006947823567315936\n",
      "Epoch 3 Batch_Num 120 Loss 0.005448030307888985\n",
      "Epoch 3 Batch_Num 121 Loss 0.0028858482837677\n",
      "Epoch 3 Batch_Num 122 Loss 0.006367218680679798\n",
      "Epoch 3 Batch_Num 123 Loss 0.004629447124898434\n",
      "Epoch 3 Batch_Num 124 Loss 0.0022369548678398132\n",
      "Epoch 3 Batch_Num 125 Loss 0.0011551628122106194\n",
      "Epoch 3 Batch_Num 126 Loss 0.008588639087975025\n",
      "Epoch 3 Batch_Num 127 Loss 0.00135717517696321\n",
      "Epoch 3 Batch_Num 128 Loss 0.001715375459752977\n",
      "Epoch 3 Batch_Num 129 Loss 0.004496116191148758\n",
      "Epoch 3 Batch_Num 130 Loss 0.0017661337042227387\n",
      "Epoch 3 Batch_Num 131 Loss 0.0018009251216426492\n",
      "Epoch 3 Batch_Num 132 Loss 0.0008357928018085659\n",
      "Epoch 3 Batch_Num 133 Loss 0.0009225805988535285\n",
      "Epoch 3 Batch_Num 134 Loss 0.002145242877304554\n",
      "Epoch 3 Batch_Num 135 Loss 0.001931099919602275\n",
      "Epoch 3 Batch_Num 136 Loss 0.005716040730476379\n",
      "Epoch 3 Batch_Num 137 Loss 0.005559719167649746\n",
      "Epoch 3 Batch_Num 138 Loss 0.0030166388023644686\n",
      "Epoch 3 Batch_Num 139 Loss 0.002805777359753847\n",
      "Epoch 3 Batch_Num 140 Loss 0.005016513634473085\n",
      "Epoch 3 Batch_Num 141 Loss 0.004251833073794842\n",
      "Epoch 3 Batch_Num 142 Loss 0.0030892828945070505\n",
      "Epoch 3 Batch_Num 143 Loss 0.004313394892960787\n",
      "Epoch 3 Batch_Num 144 Loss 0.004605686757713556\n",
      "Epoch 3 Batch_Num 145 Loss 0.001714155776426196\n",
      "Epoch 3 Batch_Num 146 Loss 0.0034146744292229414\n",
      "Epoch 3 Batch_Num 147 Loss 0.009379290044307709\n",
      "Epoch 3 Batch_Num 148 Loss 0.0021121366880834103\n",
      "Epoch 3 Batch_Num 149 Loss 0.0057747941464185715\n",
      "Epoch 3 Batch_Num 150 Loss 0.01299460232257843\n",
      "Epoch 3 Batch_Num 151 Loss 0.007888871245086193\n",
      "Epoch 3 Batch_Num 152 Loss 0.0010248168837279081\n",
      "Epoch 3 Batch_Num 153 Loss 0.012013652361929417\n",
      "Epoch 3 Batch_Num 154 Loss 0.0005240117898210883\n",
      "Epoch 3 Batch_Num 155 Loss 0.006296850740909576\n",
      "Epoch 3 Batch_Num 156 Loss 0.002986229956150055\n",
      "Epoch 3 Batch_Num 157 Loss 0.008579443208873272\n",
      "Epoch 3 Batch_Num 158 Loss 0.006136102136224508\n",
      "Epoch 3 Batch_Num 159 Loss 0.002848721109330654\n",
      "Epoch 3 Batch_Num 160 Loss 0.004262177273631096\n",
      "Epoch 3 Batch_Num 161 Loss 0.009362146258354187\n",
      "Epoch 3 Batch_Num 162 Loss 0.0046296147629618645\n",
      "Epoch 3 Batch_Num 163 Loss 0.008916321210563183\n",
      "Epoch 3 Batch_Num 164 Loss 0.00397353945299983\n",
      "Epoch 3 Batch_Num 165 Loss 0.0017572243232280016\n",
      "Epoch 3 Batch_Num 166 Loss 0.0026390738785266876\n",
      "Epoch 3 Batch_Num 167 Loss 0.0045853485353291035\n",
      "Epoch 3 Batch_Num 168 Loss 0.004914694931358099\n",
      "Epoch 3 Batch_Num 169 Loss 0.001416264334693551\n",
      "Epoch 3 Batch_Num 170 Loss 0.007559572346508503\n",
      "Epoch 3 Batch_Num 171 Loss 0.0024136563297361135\n",
      "Epoch 3 Batch_Num 172 Loss 0.0021978127770125866\n",
      "Epoch 3 Batch_Num 173 Loss 0.0037897315341979265\n",
      "Epoch 3 Batch_Num 174 Loss 0.0015468860510736704\n",
      "Epoch 3 Batch_Num 175 Loss 0.0066623082384467125\n",
      "Epoch 3 Batch_Num 176 Loss 0.0014752279967069626\n",
      "Epoch 3 Batch_Num 177 Loss 0.004507886245846748\n",
      "Epoch 3 Batch_Num 178 Loss 0.005947255529463291\n",
      "Epoch 3 Batch_Num 179 Loss 0.007276301272213459\n",
      "Epoch 3 Batch_Num 180 Loss 0.005195824429392815\n",
      "Epoch 3 Batch_Num 181 Loss 0.006163732148706913\n",
      "Epoch 3 Batch_Num 182 Loss 0.003589958418160677\n",
      "Epoch 3 Batch_Num 183 Loss 0.00045714652515016496\n",
      "Epoch 3 Batch_Num 184 Loss 0.004832753445953131\n",
      "Epoch 3 Batch_Num 185 Loss 0.002238313900306821\n",
      "Epoch 3 Batch_Num 186 Loss 0.002771656261757016\n",
      "Epoch 3 Batch_Num 187 Loss 0.002715647453442216\n",
      "Epoch 3 Batch_Num 188 Loss 0.002192078623920679\n",
      "Epoch 3 Batch_Num 189 Loss 0.004863032139837742\n",
      "Epoch 3 Batch_Num 190 Loss 0.0003756462247110903\n",
      "Epoch 3 Batch_Num 191 Loss 0.011236139573156834\n",
      "Epoch 3 Batch_Num 192 Loss 0.0013471920974552631\n",
      "Epoch 3 Batch_Num 193 Loss 0.005867018364369869\n",
      "Epoch 3 Batch_Num 194 Loss 0.0033588330261409283\n",
      "Epoch 3 Batch_Num 195 Loss 0.0037035210989415646\n",
      "Epoch 3 Batch_Num 196 Loss 0.005103101022541523\n",
      "Epoch 3 Batch_Num 197 Loss 0.0008372394368052483\n",
      "Epoch 3 Batch_Num 198 Loss 0.0007463892106898129\n",
      "Epoch 3 Batch_Num 199 Loss 0.003998796455562115\n",
      "Epoch 3 Batch_Num 200 Loss 0.005490255542099476\n",
      "Epoch 3 Batch_Num 201 Loss 0.0008593347738496959\n",
      "Epoch 3 Batch_Num 202 Loss 0.005256236530840397\n",
      "Epoch 3 Batch_Num 203 Loss 0.0037758145481348038\n",
      "Epoch 3 Batch_Num 204 Loss 0.004350568167865276\n",
      "Epoch 3 Batch_Num 205 Loss 0.0044739460572600365\n",
      "Epoch 3 Batch_Num 206 Loss 0.015740061178803444\n",
      "Epoch 3 Batch_Num 207 Loss 0.011036394163966179\n",
      "Epoch 3 Batch_Num 208 Loss 0.0015519045991823077\n",
      "Epoch 3 Batch_Num 209 Loss 0.0017211411613970995\n",
      "Epoch 3 Batch_Num 210 Loss 0.002203732030466199\n",
      "Epoch 3 Batch_Num 211 Loss 0.0023144192527979612\n",
      "Epoch 3 Batch_Num 212 Loss 0.00024430957273580134\n",
      "Epoch 3 Batch_Num 213 Loss 0.003864200320094824\n",
      "Epoch 3 Batch_Num 214 Loss 0.0026592365466058254\n",
      "Epoch 3 Batch_Num 215 Loss 0.000519410299602896\n",
      "Epoch 3 Batch_Num 216 Loss 0.006249001249670982\n",
      "Epoch 3 Batch_Num 217 Loss 0.0014500456163659692\n",
      "Epoch 3 Batch_Num 218 Loss 0.00307064363732934\n",
      "Epoch 3 Batch_Num 219 Loss 0.0016781517770141363\n",
      "Epoch 3 Batch_Num 220 Loss 0.002255083527415991\n",
      "Epoch 3 Batch_Num 221 Loss 0.003905591322109103\n",
      "Epoch 3 Batch_Num 222 Loss 0.0027317400090396404\n",
      "Epoch 3 Batch_Num 223 Loss 0.001256400952115655\n",
      "Epoch 3 Batch_Num 224 Loss 0.0030647623352706432\n",
      "Epoch 3 Batch_Num 225 Loss 0.008629601448774338\n",
      "Epoch 3 Batch_Num 226 Loss 0.0011467570438981056\n",
      "Epoch 3 Batch_Num 227 Loss 0.00029790756525471807\n",
      "Epoch 3 Batch_Num 228 Loss 0.0004767393693327904\n",
      "Epoch 3 Batch_Num 229 Loss 0.0023712669499218464\n",
      "Epoch 3 Batch_Num 230 Loss 7.050610292935744e-06\n",
      "Epoch 3 Batch_Num 231 Loss 0.0008840922964736819\n",
      "Epoch 3 Batch_Num 232 Loss 0.0013737197732552886\n",
      "Epoch 3 Batch_Num 233 Loss 0.0075835795141756535\n",
      "Epoch 3 Batch_Num 234 Loss 0.0031025963835418224\n",
      "Epoch 4 Batch_Num 0 Loss 0.003599458606913686\n",
      "Epoch 4 Batch_Num 1 Loss 0.0043725729919970036\n",
      "Epoch 4 Batch_Num 2 Loss 0.0027767294086515903\n",
      "Epoch 4 Batch_Num 3 Loss 0.011183017864823341\n",
      "Epoch 4 Batch_Num 4 Loss 0.002698680618777871\n",
      "Epoch 4 Batch_Num 5 Loss 0.004250107798725367\n",
      "Epoch 4 Batch_Num 6 Loss 0.0033094915561378\n",
      "Epoch 4 Batch_Num 7 Loss 0.001506380969658494\n",
      "Epoch 4 Batch_Num 8 Loss 0.001537985634058714\n",
      "Epoch 4 Batch_Num 9 Loss 0.0010492964647710323\n",
      "Epoch 4 Batch_Num 10 Loss 0.007344461977481842\n",
      "Epoch 4 Batch_Num 11 Loss 0.007975946180522442\n",
      "Epoch 4 Batch_Num 12 Loss 0.000960142002440989\n",
      "Epoch 4 Batch_Num 13 Loss 0.0016404504422098398\n",
      "Epoch 4 Batch_Num 14 Loss 0.0028581744991242886\n",
      "Epoch 4 Batch_Num 15 Loss 0.0019233226776123047\n",
      "Epoch 4 Batch_Num 16 Loss 0.0021782349795103073\n",
      "Epoch 4 Batch_Num 17 Loss 0.005501303356140852\n",
      "Epoch 4 Batch_Num 18 Loss 0.0025869894307106733\n",
      "Epoch 4 Batch_Num 19 Loss 0.0028327007312327623\n",
      "Epoch 4 Batch_Num 20 Loss 0.002565912436693907\n",
      "Epoch 4 Batch_Num 21 Loss 0.0016431279946118593\n",
      "Epoch 4 Batch_Num 22 Loss 0.0006914478726685047\n",
      "Epoch 4 Batch_Num 23 Loss 0.003781399456784129\n",
      "Epoch 4 Batch_Num 24 Loss 0.001934112049639225\n",
      "Epoch 4 Batch_Num 25 Loss 0.0022175367921590805\n",
      "Epoch 4 Batch_Num 26 Loss 0.000649693887680769\n",
      "Epoch 4 Batch_Num 27 Loss 0.0023120746482163668\n",
      "Epoch 4 Batch_Num 28 Loss 0.003959460649639368\n",
      "Epoch 4 Batch_Num 29 Loss 0.005419098772108555\n",
      "Epoch 4 Batch_Num 30 Loss 0.0067692166194319725\n",
      "Epoch 4 Batch_Num 31 Loss 0.0034247986041009426\n",
      "Epoch 4 Batch_Num 32 Loss 0.0072896950878202915\n",
      "Epoch 4 Batch_Num 33 Loss 0.0057706148363649845\n",
      "Epoch 4 Batch_Num 34 Loss 0.006242422852665186\n",
      "Epoch 4 Batch_Num 35 Loss 0.0014400493819266558\n",
      "Epoch 4 Batch_Num 36 Loss 0.0009163107024505734\n",
      "Epoch 4 Batch_Num 37 Loss 0.00022252909548114985\n",
      "Epoch 4 Batch_Num 38 Loss 0.00037877727299928665\n",
      "Epoch 4 Batch_Num 39 Loss 0.002323437947779894\n",
      "Epoch 4 Batch_Num 40 Loss 0.003542361781001091\n",
      "Epoch 4 Batch_Num 41 Loss 0.0006925403722561896\n",
      "Epoch 4 Batch_Num 42 Loss 0.0074725463055074215\n",
      "Epoch 4 Batch_Num 43 Loss 0.0074351197108626366\n",
      "Epoch 4 Batch_Num 44 Loss 0.0029803323559463024\n",
      "Epoch 4 Batch_Num 45 Loss 0.008151382207870483\n",
      "Epoch 4 Batch_Num 46 Loss 0.005421046633273363\n",
      "Epoch 4 Batch_Num 47 Loss 0.00042235496221110225\n",
      "Epoch 4 Batch_Num 48 Loss 0.0003400457790121436\n",
      "Epoch 4 Batch_Num 49 Loss 0.0028056122828274965\n",
      "Epoch 4 Batch_Num 50 Loss 0.00222080503590405\n",
      "Epoch 4 Batch_Num 51 Loss 0.005301568657159805\n",
      "Epoch 4 Batch_Num 52 Loss 0.00915304571390152\n",
      "Epoch 4 Batch_Num 53 Loss 0.002736705355346203\n",
      "Epoch 4 Batch_Num 54 Loss 0.0023430227302014828\n",
      "Epoch 4 Batch_Num 55 Loss 0.0007799795712344348\n",
      "Epoch 4 Batch_Num 56 Loss 0.009465784765779972\n",
      "Epoch 4 Batch_Num 57 Loss 0.0007767811184749007\n",
      "Epoch 4 Batch_Num 58 Loss 0.0005501240375451744\n",
      "Epoch 4 Batch_Num 59 Loss 0.0037306267768144608\n",
      "Epoch 4 Batch_Num 60 Loss 0.00730539858341217\n",
      "Epoch 4 Batch_Num 61 Loss 0.002836940111592412\n",
      "Epoch 4 Batch_Num 62 Loss 0.0033432189375162125\n",
      "Epoch 4 Batch_Num 63 Loss 0.002439251635223627\n",
      "Epoch 4 Batch_Num 64 Loss 0.002182023599743843\n",
      "Epoch 4 Batch_Num 65 Loss 0.002718724776059389\n",
      "Epoch 4 Batch_Num 66 Loss 0.005074565298855305\n",
      "Epoch 4 Batch_Num 67 Loss 0.0010416762670502067\n",
      "Epoch 4 Batch_Num 68 Loss 0.0036970425862818956\n",
      "Epoch 4 Batch_Num 69 Loss 0.009578150697052479\n",
      "Epoch 4 Batch_Num 70 Loss 0.005150324665009975\n",
      "Epoch 4 Batch_Num 71 Loss 0.001212111092172563\n",
      "Epoch 4 Batch_Num 72 Loss 0.0037416419945657253\n",
      "Epoch 4 Batch_Num 73 Loss 0.000891501666046679\n",
      "Epoch 4 Batch_Num 74 Loss 0.0006400495767593384\n",
      "Epoch 4 Batch_Num 75 Loss 0.007033734582364559\n",
      "Epoch 4 Batch_Num 76 Loss 0.0018243383383378386\n",
      "Epoch 4 Batch_Num 77 Loss 0.002372316550463438\n",
      "Epoch 4 Batch_Num 78 Loss 0.002517037559300661\n",
      "Epoch 4 Batch_Num 79 Loss 0.0028817085549235344\n",
      "Epoch 4 Batch_Num 80 Loss 0.0020207688212394714\n",
      "Epoch 4 Batch_Num 81 Loss 0.002608897630125284\n",
      "Epoch 4 Batch_Num 82 Loss 0.0007316231494769454\n",
      "Epoch 4 Batch_Num 83 Loss 0.0048525696620345116\n",
      "Epoch 4 Batch_Num 84 Loss 0.004595092032104731\n",
      "Epoch 4 Batch_Num 85 Loss 0.001724104629829526\n",
      "Epoch 4 Batch_Num 86 Loss 0.004588925279676914\n",
      "Epoch 4 Batch_Num 87 Loss 0.002184005919843912\n",
      "Epoch 4 Batch_Num 88 Loss 0.007238098885864019\n",
      "Epoch 4 Batch_Num 89 Loss 0.0008673485135659575\n",
      "Epoch 4 Batch_Num 90 Loss 0.00035447717527858913\n",
      "Epoch 4 Batch_Num 91 Loss 0.0009881603764370084\n",
      "Epoch 4 Batch_Num 92 Loss 0.004658996127545834\n",
      "Epoch 4 Batch_Num 93 Loss 0.003040025010704994\n",
      "Epoch 4 Batch_Num 94 Loss 0.005494431126862764\n",
      "Epoch 4 Batch_Num 95 Loss 0.007838388904929161\n",
      "Epoch 4 Batch_Num 96 Loss 0.006321323104202747\n",
      "Epoch 4 Batch_Num 97 Loss 0.003265645820647478\n",
      "Epoch 4 Batch_Num 98 Loss 0.0021864220034331083\n",
      "Epoch 4 Batch_Num 99 Loss 0.0052903397008776665\n",
      "Epoch 4 Batch_Num 100 Loss 0.0009959822054952383\n",
      "Epoch 4 Batch_Num 101 Loss 0.0009841830469667912\n",
      "Epoch 4 Batch_Num 102 Loss 0.00026329595129936934\n",
      "Epoch 4 Batch_Num 103 Loss 0.009475676342844963\n",
      "Epoch 4 Batch_Num 104 Loss 0.011963226832449436\n",
      "Epoch 4 Batch_Num 105 Loss 0.005481182597577572\n",
      "Epoch 4 Batch_Num 106 Loss 0.0006126969819888473\n",
      "Epoch 4 Batch_Num 107 Loss 0.001979256048798561\n",
      "Epoch 4 Batch_Num 108 Loss 0.005765334703028202\n",
      "Epoch 4 Batch_Num 109 Loss 0.007097250781953335\n",
      "Epoch 4 Batch_Num 110 Loss 0.009891385212540627\n",
      "Epoch 4 Batch_Num 111 Loss 0.002105921972543001\n",
      "Epoch 4 Batch_Num 112 Loss 0.008156866766512394\n",
      "Epoch 4 Batch_Num 113 Loss 0.006280621979385614\n",
      "Epoch 4 Batch_Num 114 Loss 0.0032036001794040203\n",
      "Epoch 4 Batch_Num 115 Loss 0.004822584800422192\n",
      "Epoch 4 Batch_Num 116 Loss 0.0047559598460793495\n",
      "Epoch 4 Batch_Num 117 Loss 0.0011928777676075697\n",
      "Epoch 4 Batch_Num 118 Loss 0.0008998309494927526\n",
      "Epoch 4 Batch_Num 119 Loss 0.001087859971448779\n",
      "Epoch 4 Batch_Num 120 Loss 0.003992798738181591\n",
      "Epoch 4 Batch_Num 121 Loss 0.00696539506316185\n",
      "Epoch 4 Batch_Num 122 Loss 0.0010972149902954698\n",
      "Epoch 4 Batch_Num 123 Loss 0.009657617658376694\n",
      "Epoch 4 Batch_Num 124 Loss 0.0020469597075134516\n",
      "Epoch 4 Batch_Num 125 Loss 0.004531999584287405\n",
      "Epoch 4 Batch_Num 126 Loss 0.009251673705875874\n",
      "Epoch 4 Batch_Num 127 Loss 0.001154850935563445\n",
      "Epoch 4 Batch_Num 128 Loss 0.004645361099392176\n",
      "Epoch 4 Batch_Num 129 Loss 0.0004300685541238636\n",
      "Epoch 4 Batch_Num 130 Loss 0.0018518110737204552\n",
      "Epoch 4 Batch_Num 131 Loss 0.0011522772256284952\n",
      "Epoch 4 Batch_Num 132 Loss 0.00044033798621967435\n",
      "Epoch 4 Batch_Num 133 Loss 0.0016615975182503462\n",
      "Epoch 4 Batch_Num 134 Loss 0.003641880350187421\n",
      "Epoch 4 Batch_Num 135 Loss 0.008310748264193535\n",
      "Epoch 4 Batch_Num 136 Loss 0.008670160546898842\n",
      "Epoch 4 Batch_Num 137 Loss 0.0045278724282979965\n",
      "Epoch 4 Batch_Num 138 Loss 0.004510893020778894\n",
      "Epoch 4 Batch_Num 139 Loss 0.001047655357979238\n",
      "Epoch 4 Batch_Num 140 Loss 0.0018894418608397245\n",
      "Epoch 4 Batch_Num 141 Loss 0.0038433962035924196\n",
      "Epoch 4 Batch_Num 142 Loss 0.003714656922966242\n",
      "Epoch 4 Batch_Num 143 Loss 0.004010227508842945\n",
      "Epoch 4 Batch_Num 144 Loss 0.007058127783238888\n",
      "Epoch 4 Batch_Num 145 Loss 0.00043522854684852064\n",
      "Epoch 4 Batch_Num 146 Loss 0.004653800278902054\n",
      "Epoch 4 Batch_Num 147 Loss 0.007518846541643143\n",
      "Epoch 4 Batch_Num 148 Loss 0.0003592197026591748\n",
      "Epoch 4 Batch_Num 149 Loss 0.002510427962988615\n",
      "Epoch 4 Batch_Num 150 Loss 0.0041139088571071625\n",
      "Epoch 4 Batch_Num 151 Loss 0.009008901193737984\n",
      "Epoch 4 Batch_Num 152 Loss 0.00032207451295107603\n",
      "Epoch 4 Batch_Num 153 Loss 0.006002151872962713\n",
      "Epoch 4 Batch_Num 154 Loss 0.00260140304453671\n",
      "Epoch 4 Batch_Num 155 Loss 0.0058605968952178955\n",
      "Epoch 4 Batch_Num 156 Loss 0.002470831386744976\n",
      "Epoch 4 Batch_Num 157 Loss 0.0061248173005878925\n",
      "Epoch 4 Batch_Num 158 Loss 0.0013185604475438595\n",
      "Epoch 4 Batch_Num 159 Loss 0.001840636134147644\n",
      "Epoch 4 Batch_Num 160 Loss 0.0020265658386051655\n",
      "Epoch 4 Batch_Num 161 Loss 0.003705417737364769\n",
      "Epoch 4 Batch_Num 162 Loss 0.0030437326058745384\n",
      "Epoch 4 Batch_Num 163 Loss 0.004545250441879034\n",
      "Epoch 4 Batch_Num 164 Loss 0.007810855749994516\n",
      "Epoch 4 Batch_Num 165 Loss 0.0016701702261343598\n",
      "Epoch 4 Batch_Num 166 Loss 0.003185345558449626\n",
      "Epoch 4 Batch_Num 167 Loss 0.002390271984040737\n",
      "Epoch 4 Batch_Num 168 Loss 0.002739065559580922\n",
      "Epoch 4 Batch_Num 169 Loss 0.0008895453065633774\n",
      "Epoch 4 Batch_Num 170 Loss 0.0019505990203469992\n",
      "Epoch 4 Batch_Num 171 Loss 0.002983508864417672\n",
      "Epoch 4 Batch_Num 172 Loss 0.0026871240697801113\n",
      "Epoch 4 Batch_Num 173 Loss 0.006683063693344593\n",
      "Epoch 4 Batch_Num 174 Loss 0.0005433456972241402\n",
      "Epoch 4 Batch_Num 175 Loss 0.002769163344055414\n",
      "Epoch 4 Batch_Num 176 Loss 0.006454680114984512\n",
      "Epoch 4 Batch_Num 177 Loss 0.006031516008079052\n",
      "Epoch 4 Batch_Num 178 Loss 0.0009022399899549782\n",
      "Epoch 4 Batch_Num 179 Loss 0.003930217586457729\n",
      "Epoch 4 Batch_Num 180 Loss 0.0050553446635603905\n",
      "Epoch 4 Batch_Num 181 Loss 0.0027479236014187336\n",
      "Epoch 4 Batch_Num 182 Loss 0.0010897456668317318\n",
      "Epoch 4 Batch_Num 183 Loss 0.0012023269664496183\n",
      "Epoch 4 Batch_Num 184 Loss 0.0060928077436983585\n",
      "Epoch 4 Batch_Num 185 Loss 0.004695283714681864\n",
      "Epoch 4 Batch_Num 186 Loss 0.005900143645703793\n",
      "Epoch 4 Batch_Num 187 Loss 0.0030463512521237135\n",
      "Epoch 4 Batch_Num 188 Loss 0.0013437988236546516\n",
      "Epoch 4 Batch_Num 189 Loss 0.007555438671261072\n",
      "Epoch 4 Batch_Num 190 Loss 0.00045420145033858716\n",
      "Epoch 4 Batch_Num 191 Loss 0.003661051392555237\n",
      "Epoch 4 Batch_Num 192 Loss 0.0006970410468056798\n",
      "Epoch 4 Batch_Num 193 Loss 0.0014686387730762362\n",
      "Epoch 4 Batch_Num 194 Loss 0.002197631634771824\n",
      "Epoch 4 Batch_Num 195 Loss 0.0013906043022871017\n",
      "Epoch 4 Batch_Num 196 Loss 0.0018860946875065565\n",
      "Epoch 4 Batch_Num 197 Loss 0.0014089858159422874\n",
      "Epoch 4 Batch_Num 198 Loss 0.0005167179042473435\n",
      "Epoch 4 Batch_Num 199 Loss 0.001593159744516015\n",
      "Epoch 4 Batch_Num 200 Loss 0.003950062673538923\n",
      "Epoch 4 Batch_Num 201 Loss 0.0055188448168337345\n",
      "Epoch 4 Batch_Num 202 Loss 0.003925262484699488\n",
      "Epoch 4 Batch_Num 203 Loss 0.0020725089125335217\n",
      "Epoch 4 Batch_Num 204 Loss 0.0016603799303993583\n",
      "Epoch 4 Batch_Num 205 Loss 0.006231148261576891\n",
      "Epoch 4 Batch_Num 206 Loss 0.01297470461577177\n",
      "Epoch 4 Batch_Num 207 Loss 0.00508258817717433\n",
      "Epoch 4 Batch_Num 208 Loss 0.0018712632590904832\n",
      "Epoch 4 Batch_Num 209 Loss 0.0016401454340666533\n",
      "Epoch 4 Batch_Num 210 Loss 0.00294091715477407\n",
      "Epoch 4 Batch_Num 211 Loss 0.004019488580524921\n",
      "Epoch 4 Batch_Num 212 Loss 0.0017685589846223593\n",
      "Epoch 4 Batch_Num 213 Loss 0.004182774573564529\n",
      "Epoch 4 Batch_Num 214 Loss 0.0016594936605542898\n",
      "Epoch 4 Batch_Num 215 Loss 0.0024046117905527353\n",
      "Epoch 4 Batch_Num 216 Loss 0.0037659797817468643\n",
      "Epoch 4 Batch_Num 217 Loss 0.0022409379016608\n",
      "Epoch 4 Batch_Num 218 Loss 0.005022138357162476\n",
      "Epoch 4 Batch_Num 219 Loss 0.0022706494200974703\n",
      "Epoch 4 Batch_Num 220 Loss 0.0012238869676366448\n",
      "Epoch 4 Batch_Num 221 Loss 0.0035059538204222918\n",
      "Epoch 4 Batch_Num 222 Loss 0.00021839069086126983\n",
      "Epoch 4 Batch_Num 223 Loss 0.0018156444421038032\n",
      "Epoch 4 Batch_Num 224 Loss 0.0015460614813491702\n",
      "Epoch 4 Batch_Num 225 Loss 0.008318021893501282\n",
      "Epoch 4 Batch_Num 226 Loss 0.005888273939490318\n",
      "Epoch 4 Batch_Num 227 Loss 8.895847713574767e-05\n",
      "Epoch 4 Batch_Num 228 Loss 0.0015208430122584105\n",
      "Epoch 4 Batch_Num 229 Loss 0.001611935207620263\n",
      "Epoch 4 Batch_Num 230 Loss 0.0003927487414330244\n",
      "Epoch 4 Batch_Num 231 Loss 0.0018489282811060548\n",
      "Epoch 4 Batch_Num 232 Loss 0.0009652081062085927\n",
      "Epoch 4 Batch_Num 233 Loss 0.0027383328415453434\n",
      "Epoch 4 Batch_Num 234 Loss 0.0027680504135787487\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for f in range(num_epochs):\n",
    "    for batch_num, minibatch in enumerate(train_loader):\n",
    "        minibatch_x, minibatch_y = minibatch[0], minibatch[1]\n",
    "        output = model.forward(torch.Tensor(minibatch_x.float()))\n",
    "        loss = criterion(output, torch.Tensor(minibatch_y.float()))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {f} Batch_Num {batch_num} Loss {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/20 23:31:09 INFO mlflow.tracking.fluent: Experiment with name 'PyTorch_MNIST' does not exist. Creating a new experiment.\n",
      "/var/folders/b0/lvr6c0md2rb5v_rb1ngcc27h0000gn/T/ipykernel_10277/2201976014.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = nn.Softmax()(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_acc:  0.9789\n",
      "auc_score:  0.9883053250821299\n",
      "CPU times: user 1.91 s, sys: 681 ms, total: 2.59 s\n",
      "Wall time: 4.27 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alimokh/opt/miniconda3/envs/torch/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mlflow.set_experiment(\"PyTorch_MNIST\") \n",
    "with mlflow.start_run():\n",
    "    preds = model.forward(torch.Tensor(x_test.float()))\n",
    "    preds = np.round(preds.detach().cpu().numpy())\n",
    "    eval_acc = accuracy_score(y_test, preds)\n",
    "    auc_score = roc_auc_score(y_test, preds)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "    mlflow.log_metric(\"eval_acc\", eval_acc)\n",
    "    mlflow.log_metric(\"auc_score\", auc_score)\n",
    "    print(\"eval_acc: \", eval_acc)\n",
    "    print(\"auc_score: \", auc_score)\n",
    "    mlflow.pytorch.log_model(model, \"PyTorch_MNIST\")\n",
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
